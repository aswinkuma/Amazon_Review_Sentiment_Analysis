{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                                Get the large set\n",
       "rating                                                             3.0\n",
       "body                 go ahead and buy the large set the small set o...\n",
       "y                                                                    1\n",
       "cleaned_reviews      go ahead buy larg set small set paperback imoi...\n",
       "classes                                                       moderate\n",
       "cleaned_reviews_1    ['ahead', 'buy', 'larg', 'set', 'small', 'set'...\n",
       "polarity                                                          -0.3\n",
       "polarity_class                                                negative\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reviews[reviews['classes']=='good']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perfect for my airport extreme router wall mount'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='polarity_class', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=reviews['polarity_class'], palette='GnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='classes', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=reviews['classes'], palette='GnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good        79\n",
       "bad         68\n",
       "moderate    58\n",
       "Name: classes, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['classes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance mesurement\n",
    "\n",
    "def model_performane(y_test, y_pred):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = reviews['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x):\n",
    "    if x == 'good':\n",
    "        return 0\n",
    "    elif x== 'moderate':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text into vectors\n",
    "cv = CountVectorizer(max_features = 909)\n",
    "\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = reviews['classes'].apply(lambda x: conv(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 909)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "gaussNB_classifier = GaussianNB()\n",
    "gaussNB_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gaussNB_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  2  3]\n",
      " [ 0  4  0]\n",
      " [ 1  8  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.75      0.83        20\n",
      "           1       0.29      1.00      0.44         4\n",
      "           2       0.73      0.47      0.57        17\n",
      "\n",
      "    accuracy                           0.66        41\n",
      "   macro avg       0.65      0.74      0.62        41\n",
      "weighted avg       0.79      0.66      0.69        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axs = plt.subplots(1,2, figsize=(9,4))\n",
    "f.suptitle(\"Predicted VS Actual\")\n",
    "\n",
    "sns.countplot(x=y_pred, ax=axs[0], palette='Blues') #countplot for predicted values\n",
    "sns.countplot(x=y_test, ax=axs[1], palette='RdPu') # countplot for actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  2]\n",
      " [ 0  2  2]\n",
      " [ 0  8  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        20\n",
      "           1       0.20      0.50      0.29         4\n",
      "           2       0.69      0.53      0.60        17\n",
      "\n",
      "    accuracy                           0.71        41\n",
      "   macro avg       0.63      0.64      0.61        41\n",
      "weighted avg       0.79      0.71      0.74        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiNB_classifier = MultinomialNB()\n",
    "multiNB_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_predNB = multiNB_classifier.predict(x_test)\n",
    "model_performane(y_test,y_predNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axs = plt.subplots(1,2, figsize=(9,4))\n",
    "f.suptitle(\"Predicted VS Actual\")\n",
    "\n",
    "sns.countplot(x=y_predNB, ax=axs[0], palette='GnBu_r') #countplot for predicted values\n",
    "sns.countplot(x=y_test, ax=axs[1], palette='RdPu') # countplot for actual values\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFclassifier = RandomForestClassifier(n_estimators=10, criterion='gini', random_state=0)\n",
    "RFclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFy_pred = RFclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  3 11]\n",
      " [ 1 27  0]\n",
      " [ 3  1  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.69      0.39      0.50        23\n",
      "        good       0.87      0.96      0.92        28\n",
      "    moderate       0.39      0.64      0.48        11\n",
      "\n",
      "    accuracy                           0.69        62\n",
      "   macro avg       0.65      0.66      0.63        62\n",
      "weighted avg       0.72      0.69      0.68        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,RFy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axs = plt.subplots(1,2, figsize=(9,4))\n",
    "f.suptitle(\"Predicted VS Actual\")\n",
    "\n",
    "sns.countplot(x=RFy_pred, ax=axs[0], palette='Blues') #countplot for predicted values\n",
    "sns.countplot(x=y_test, ax=axs[1], palette='RdPu') # countplot for actual values\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "logisticreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticreg_pred = logisticreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  1  7]\n",
      " [ 2 26  0]\n",
      " [ 4  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.71      0.65      0.68        23\n",
      "        good       0.96      0.93      0.95        28\n",
      "    moderate       0.50      0.64      0.56        11\n",
      "\n",
      "    accuracy                           0.77        62\n",
      "   macro avg       0.73      0.74      0.73        62\n",
      "weighted avg       0.79      0.77      0.78        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,logisticreg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axs = plt.subplots(1,2, figsize=(9,4))\n",
    "f.suptitle(\"Predicted VS Actual\")\n",
    "\n",
    "sns.countplot(x=logisticreg_pred, ax=axs[0], palette='Blues') #countplot for predicted values\n",
    "sns.countplot(x=y_test, ax=axs[1], palette='RdPu') # countplot for actual values\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.742 (0.091)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.80952381, 0.85714286, 0.85714286, 0.57142857,\n",
       "       0.85      , 0.7       , 0.75      , 0.7       , 0.85      ,\n",
       "       0.76190476, 0.76190476, 0.85714286, 0.71428571, 0.76190476,\n",
       "       0.65      , 0.7       , 0.65      , 0.8       , 0.9       ,\n",
       "       0.71428571, 0.61904762, 0.61904762, 0.76190476, 0.66666667,\n",
       "       0.8       , 0.6       , 0.7       , 0.9       , 0.65      ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb = LGBMClassifier()\n",
    "lgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_pred = lgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  4  7]\n",
      " [ 0  6  5]\n",
      " [ 1 10 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.61      0.74        28\n",
      "           1       0.30      0.55      0.39        11\n",
      "           2       0.50      0.52      0.51        23\n",
      "\n",
      "    accuracy                           0.56        62\n",
      "   macro avg       0.58      0.56      0.55        62\n",
      "weighted avg       0.67      0.56      0.59        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,lgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bytree=1, enable_categorical=False, gamma=0,\n",
       "                gpu_id=-1, importance_type=None, interaction_constraints='',\n",
       "                max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "                num_parallel_tree=100, objective='multi:softprob',\n",
       "                predictor='auto', random_state=0, reg_alpha=0,\n",
       "                scale_pos_weight=None, tree_method='exact',\n",
       "                validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRFClassifier()\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  2]\n",
      " [ 0  2  9]\n",
      " [ 2  9 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        28\n",
      "           1       0.18      0.18      0.18        11\n",
      "           2       0.52      0.52      0.52        23\n",
      "\n",
      "    accuracy                           0.65        62\n",
      "   macro avg       0.54      0.54      0.54        62\n",
      "weighted avg       0.65      0.65      0.65        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_pred = adaboost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 23]\n",
      " [ 0  2 26]\n",
      " [ 1  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.00      0.00      0.00        23\n",
      "        good       1.00      0.07      0.13        28\n",
      "    moderate       0.17      0.91      0.29        11\n",
      "\n",
      "    accuracy                           0.19        62\n",
      "   macro avg       0.39      0.33      0.14        62\n",
      "weighted avg       0.48      0.19      0.11        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,adaboost_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  1]\n",
      " [ 1  2  1]\n",
      " [ 0  5 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.29      0.50      0.36         4\n",
      "           2       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.80        41\n",
      "   macro avg       0.70      0.72      0.70        41\n",
      "weighted avg       0.85      0.80      0.82        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "svc_pred = svc.predict(x_test)\n",
    "\n",
    "model_performane(y_test, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('svmodel.pkl', 'wb') as pkl:\n",
    "    pickle.dump(svc, pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000, 10000], 'gamma': [1,0.1,0.01,0.001,0.0001,0.00001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid  = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=1e-05, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.500 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=1e-05, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.759 total time=   0.0s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.607 total time=   0.0s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=1e-05, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.759 total time=   0.0s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.607 total time=   0.0s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=1e-05, kernel=rbf;, score=0.345 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=1e-05, kernel=rbf;, score=0.379 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=1e-05, kernel=rbf;, score=0.357 total time=   0.0s\n",
      "[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.552 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.759 total time=   0.0s\n",
      "[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.607 total time=   0.0s\n",
      "[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END ...C=1000, gamma=1e-05, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 2/5] END ...C=1000, gamma=1e-05, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 3/5] END ...C=1000, gamma=1e-05, kernel=rbf;, score=0.483 total time=   0.0s\n",
      "[CV 4/5] END ...C=1000, gamma=1e-05, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 5/5] END ...C=1000, gamma=1e-05, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 1/5] END ......C=10000, gamma=1, kernel=rbf;, score=0.414 total time=   0.0s\n",
      "[CV 2/5] END ......C=10000, gamma=1, kernel=rbf;, score=0.448 total time=   0.0s\n",
      "[CV 3/5] END ......C=10000, gamma=1, kernel=rbf;, score=0.552 total time=   0.0s\n",
      "[CV 4/5] END ......C=10000, gamma=1, kernel=rbf;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ......C=10000, gamma=1, kernel=rbf;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END ....C=10000, gamma=0.1, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 2/5] END ....C=10000, gamma=0.1, kernel=rbf;, score=0.759 total time=   0.0s\n",
      "[CV 3/5] END ....C=10000, gamma=0.1, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 4/5] END ....C=10000, gamma=0.1, kernel=rbf;, score=0.607 total time=   0.0s\n",
      "[CV 5/5] END ....C=10000, gamma=0.1, kernel=rbf;, score=0.643 total time=   0.0s\n",
      "[CV 1/5] END ...C=10000, gamma=0.01, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END ...C=10000, gamma=0.01, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END ...C=10000, gamma=0.01, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 4/5] END ...C=10000, gamma=0.01, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END ...C=10000, gamma=0.01, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END ..C=10000, gamma=0.001, kernel=rbf;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END ..C=10000, gamma=0.001, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END ..C=10000, gamma=0.001, kernel=rbf;, score=0.655 total time=   0.0s\n",
      "[CV 4/5] END ..C=10000, gamma=0.001, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END ..C=10000, gamma=0.001, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END .C=10000, gamma=0.0001, kernel=rbf;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END .C=10000, gamma=0.0001, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 3/5] END .C=10000, gamma=0.0001, kernel=rbf;, score=0.690 total time=   0.0s\n",
      "[CV 4/5] END .C=10000, gamma=0.0001, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END .C=10000, gamma=0.0001, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 1/5] END ..C=10000, gamma=1e-05, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END ..C=10000, gamma=1e-05, kernel=rbf;, score=0.724 total time=   0.0s\n",
      "[CV 3/5] END ..C=10000, gamma=1e-05, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END ..C=10000, gamma=1e-05, kernel=rbf;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END ..C=10000, gamma=1e-05, kernel=rbf;, score=0.643 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000, 10000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 1e-05],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  6]\n",
      " [ 6  7  8]\n",
      " [ 3  4  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72        25\n",
      "           1       0.64      0.33      0.44        21\n",
      "           2       0.39      0.56      0.46        16\n",
      "\n",
      "    accuracy                           0.56        62\n",
      "   macro avg       0.57      0.55      0.54        62\n",
      "weighted avg       0.59      0.56      0.56        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_pred, grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier()"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  1  3]\n",
      " [ 0  8  3]\n",
      " [ 2  5 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89        28\n",
      "           1       0.57      0.73      0.64        11\n",
      "           2       0.73      0.70      0.71        23\n",
      "\n",
      "    accuracy                           0.77        62\n",
      "   macro avg       0.74      0.76      0.75        62\n",
      "weighted avg       0.79      0.77      0.78        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test, ridge_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_pred = dtree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  2]\n",
      " [ 0  1 10]\n",
      " [ 1  9 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95        28\n",
      "           1       0.10      0.09      0.10        11\n",
      "           2       0.52      0.57      0.54        23\n",
      "\n",
      "    accuracy                           0.65        62\n",
      "   macro avg       0.53      0.53      0.53        62\n",
      "weighted avg       0.65      0.65      0.64        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test, dtree_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data = reviews.loc[:, 'cleaned_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index:\n",
      " {'<UNK>': 1, 'book': 2, 'great': 3, 'look': 4, 'hold': 5, 'shelf': 6, 'wall': 7, 'use': 8, 'one': 9, 'work': 10, 'screw': 11, 'instal': 12, 'size': 13, 'fit': 14, 'get': 15, 'easi': 16, 'put': 17, 'larg': 18, 'need': 19, 'would': 20, 'like': 21, 'bought': 22, 'perfect': 23, 'product': 24, 'shelv': 25, 'realli': 26, 'cover': 27, 'bottom': 28, 'weight': 29, 'stack': 30, 'idea': 31, 'well': 32, 'dont': 33, 'anchor': 34, 'make': 35, 'float': 36, 'even': 37, 'item': 38, 'enough': 39, 'im': 40, 'larger': 41, 'order': 42, 'could': 43, 'hardwar': 44, 'bookshelf': 45, 'standard': 46, 'keep': 47, 'want': 48, 'mount': 49, 'much': 50, 'come': 51, 'singl': 52, 'way': 53, 'buy': 54, 'hardcov': 55, 'go': 56, 'big': 57, 'reach': 58, 'thing': 59, 'smaller': 60, 'expect': 61, 'purchas': 62, 'lb': 63, 'set': 64, 'small': 65, 'doesnt': 66, 'tri': 67, 'place': 68, 'disappoint': 69, 'bend': 70, 'good': 71, 'think': 72, 'pretti': 73, 'also': 74, 'didnt': 75, 'hole': 76, 'littl': 77, 'hous': 78, 'see': 79, 'includ': 80, 'differ': 81, 'stud': 82, 'design': 83, 'hardback': 84, 'love': 85, 'though': 86, 'metal': 87, 'cookbook': 88, 'flimsi': 89, 'display': 90, 'room': 91, 'cool': 92, 'back': 93, 'bracket': 94, 'far': 95, 'hang': 96, 'fall': 97, 'wish': 98, 'three': 99, 'lot': 100, 'said': 101, 'seem': 102, 'two': 103, 'doubl': 104, 'space': 105, 'came': 106, 'still': 107, 'exactli': 108, 'qualiti': 109, 'less': 110, 'feel': 111, 'normal': 112, 'fine': 113, 'wide': 114, 'conceal': 115, 'hung': 116, 'read': 117, 'actual': 118, 'return': 119, 'howev': 120, 'top': 121, 'hard': 122, 'easili': 123, 'review': 124, 'nice': 125, 'support': 126, 'packag': 127, 'piec': 128, 'made': 129, 'arriv': 130, 'receiv': 131, 'say': 132, 'scratch': 133, 'time': 134, 'seller': 135, 'cant': 136, 'lip': 137, 'brand': 138, 'abl': 139, 'miss': 140, 'right': 141, 'hook': 142, 'pound': 143, 'recommend': 144, 'bit': 145, 'base': 146, 'mani': 147, 'wasnt': 148, 'pictur': 149, 'end': 150, 'wouldnt': 151, 'know': 152, 'bookshelv': 153, 'problem': 154, 'whole': 155, 'describ': 156, 'strong': 157, 'version': 158, 'awesom': 159, 'sturdi': 160, 'depth': 161, 'sinc': 162, 'around': 163, 'happi': 164, 'paperback': 165, 'invis': 166, 'find': 167, 'thought': 168, 'found': 169, 'descript': 170, 'stay': 171, 'unit': 172, 'star': 173, 'sever': 174, 'long': 175, 'realiz': 176, 'limit': 177, 'kitchen': 178, 'part': 179, 'instead': 180, 'instruct': 181, 'heavier': 182, 'definit': 183, 'claim': 184, 'isnt': 185, 'guess': 186, 'textbook': 187, 'hope': 188, 'year': 189, 'home': 190, 'measur': 191, 'sag': 192, 'rather': 193, 'novel': 194, 'price': 195, 'matter': 196, 'complet': 197, 'kind': 198, 'bigger': 199, 'got': 200, 'box': 201, 'ive': 202, 'photo': 203, 'collect': 204, 'addit': 205, 'bent': 206, 'amazon': 207, 'check': 208, 'carri': 209, 'show': 210, 'heavi': 211, 'uniqu': 212, 'store': 213, 'ask': 214, 'fell': 215, 'attach': 216, 'amaz': 217, 'daughter': 218, 'downward': 219, 'secur': 220, 'start': 221, 'job': 222, 'close': 223, 'otherwis': 224, 'dimens': 225, 'son': 226, 'case': 227, 'state': 228, 'true': 229, 'issu': 230, 'may': 231, 'drill': 232, 'other': 233, 'compar': 234, 'high': 235, 'stabl': 236, 'inch': 237, 'concept': 238, 'wont': 239, 'insid': 240, 'open': 241, 'manufactur': 242, 'pack': 243, 'individu': 244, 'new': 245, 'anyth': 246, 'second': 247, 'advertis': 248, 'money': 249, 'pleas': 250, 'better': 251, 'ok': 252, 'umbra': 253, 'speaker': 254, 'cannot': 255, 'load': 256, 'lightweight': 257, 'head': 258, 'drywal': 259, 'plate': 260, 'save': 261, 'side': 262, 'offic': 263, 'favorit': 264, 'poor': 265, 'none': 266, 'custom': 267, 'servic': 268, 'exchang': 269, 'sure': 270, 'except': 271, 'point': 272, 'towel': 273, 'slide': 274, 'singlewid': 275, 'tell': 276, 'beauti': 277, 'best': 278, 'neat': 279, 'wonder': 280, 'excel': 281, 'call': 282, 'max': 283, 'slope': 284, 'ahead': 285, 'anoth': 286, 'effect': 287, 'excit': 288, 'suppos': 289, 'scare': 290, 'might': 291, 'impress': 292, 'quit': 293, 'specif': 294, 'slip': 295, 'super': 296, 'main': 297, 'underneath': 298, 'basic': 299, 'defeat': 300, 'purpos': 301, 'meant': 302, 'first': 303, 'term': 304, 'plan': 305, 'tacki': 306, 'sort': 307, 'soft': 308, 'stick': 309, 'comfort': 310, 'ever': 311, 'across': 312, 'amount': 313, 'rate': 314, 'take': 315, 'extrem': 316, 'type': 317, 'school': 318, 'wider': 319, 'extra': 320, 'cut': 321, 'width': 322, 'straight': 323, 'felt': 324, 'gotten': 325, 'multipl': 326, 'horizont': 327, 'arent': 328, 'center': 329, 'harri': 330, 'potter': 331, 'hidden': 332, 'upset': 333, 'mislead': 334, 'sound': 335, 'bounc': 336, 'text': 337, 'touch': 338, 'without': 339, 'usual': 340, 'absolut': 341, 'particular': 342, 'promis': 343, 'extend': 344, 'forward': 345, 'believ': 346, 'test': 347, 'move': 348, 'consid': 349, 'least': 350, 'fix': 351, 'quick': 352, 'solut': 353, 'weightsiz': 354, 'correct': 355, 'huge': 356, 'meet': 357, 'suffici': 358, 'provid': 359, 'experi': 360, 'expens': 361, 'practic': 362, 'overal': 363, 'nearli': 364, 'decor': 365, 'broke': 366, 'took': 367, 'light': 368, 'difficult': 369, 'someth': 370, 'reliabl': 371, 'remaind': 372, 'poorli': 373, 'ago': 374, 'plastic': 375, 'couldnt': 376, 'screwdriv': 377, 'substanti': 378, 'frustrat': 379, 'tini': 380, 'correctli': 381, 'mayb': 382, 'lower': 383, 'your': 384, 'cheaper': 385, 'alreadi': 386, 'let': 387, 'current': 388, 'damag': 389, 'ill': 390, 'intend': 391, 'turn': 392, 'project': 393, 'pain': 394, 'chip': 395, 'section': 396, 'bang': 397, 'bill': 398, 'peopl': 399, 'coolest': 400, 'seen': 401, 'kid': 402, 'function': 403, 'clip': 404, 'pile': 405, 'busi': 406, 'nightstand': 407, 'lamp': 408, 'therefor': 409, 'board': 410, 'necessari': 411, 'happen': 412, 'highli': 413, 'within': 414, 'theyr': 415, 'yet': 416, 'typic': 417, 'spot': 418, 'area': 419, 'alway': 420, 'thank': 421, 'allow': 422, 'tv': 423, 'comment': 424, 'held': 425, 'compliment': 426, 'lol': 427, 'gift': 428, 'earli': 429, 'convers': 430, 'starter': 431, 'signific': 432, 'left': 433, 'uncomfort': 434, 'especi': 435, 'spun': 436, 'l': 437, 'imoi': 438, 'wobblesolid': 439, 'thin': 440, 'bookhold': 441, 'platform': 442, 'overs': 443, 'optioni': 444, 'lesson': 445, 'learn': 446, 'stock': 447, 'forewarn': 448, 'tricki': 449, 'prong': 450, 'undersid': 451, 'skew': 452, 'everyday': 453, 'child': 454, 'eventu': 455, 'accommod': 456, 'technic': 457, 'deep': 458, 'differenti': 459, 'bum': 460, 'salti': 461, 'climat': 462, 'give': 463, 'length': 464, 'softcov': 465, 'ad': 466, 'besid': 467, 'bed': 468, 'couch': 469, 'self': 470, 'regular': 471, 'cheap': 472, 'print': 473, 'gotta': 474, 'hit': 475, 'bottomback': 476, 'elsewher': 477, 'tear': 478, 'foundat': 479, 'quantiti': 480, 'restrict': 481, 'silver': 482, 'arm': 483, 'weak': 484, 'dri': 485, 'sheet': 486, 'paper': 487, 'onto': 488, 'regardless': 489, 'awkwardli': 490, 'bookcas': 491, 'havent': 492, 'assum': 493, 'mistak': 494, 'ponder': 495, 'avail': 496, 'unabl': 497, 'offset': 498, 'weighti': 499, 'four': 500, 'page': 501, 'verifi': 502, 'lenght': 503, 'wich': 504, 'sadli': 505, 'patch': 506, 'bummer': 507, 'tab': 508, 'spine': 509, 'confid': 510, 'everyth': 511, 'noth': 512, 'uneven': 513, 'opt': 514, 'longer': 515, 'reinforc': 516, 'outer': 517, 'involv': 518, 'itd': 519, 'easier': 520, 'graphic': 521, 'illustr': 522, 'reason': 523, 'curiou': 524, 'anyon': 525, 'els': 526, 'costli': 527, 'cost': 528, 'retail': 529, 'per': 530, 'color': 531, 'cheapest': 532, 'unstabl': 533, 'flimsier': 534, 'lean': 535, 'featur': 536, 'art': 537, 'junk': 538, 'afraid': 539, 'tad': 540, 'write': 541, 'thicker': 542, 'counter': 543, 'sunk': 544, 'help': 545, 'flush': 546, 'husband': 547, 'aww': 548, 'gorgeou': 549, 'decent': 550, 'suck': 551, 'consum': 552, 'done': 553, 'figur': 554, 'spend': 555, 'match': 556, 'behind': 557, 'applic': 558, 'flaw': 559, 'workbut': 560, 'reus': 561, 'sturdiest': 562, 'materi': 563, 'warn': 564, 'imo': 565, 'sit': 566, 'awkward': 567, 'flow': 568, 'minimum': 569, 'hollow': 570, 'hanger': 571, 'own': 572, 'ideal': 573, 'altogeth': 574, 'spread': 575, 'accur': 576, 'theori': 577, 'thu': 578, 'dump': 579, 'floor': 580, 'unceremoni': 581, 'ye': 582, 'known': 583, 'reader': 584, 'half': 585, 'plane': 586, 'rang': 587, 'coffe': 588, 'tabl': 589, 'clumsi': 590, 'rubber': 591, 'band': 592, 'trustworthi': 593, 'ship': 594, 'supposedli': 595, 'inferior': 596, 'surround': 597, 'onlycamewith': 598, 'sometim': 599, 'tilt': 600, 'finish': 601, 'worn': 602, 'anywher': 603, 'near': 604, 'coupl': 605, 'flex': 606, 'offer': 607, 'discount': 608, 'upon': 609, 'final': 610, 'backer': 611, 'rod': 612, 'prevent': 613, 'okay': 614, 'studi': 615, 'bump': 616, 'tho': 617, 'kick': 618, 'displayand': 619, 'valu': 620, 'unfortun': 621, 'obviou': 622, 'kit': 623, 'shouldv': 624, 'aw': 625, 'care': 626, 'direct': 627, 'togeth': 628, 'month': 629, 'themhowev': 630, 'last': 631, 'jacket': 632, 'whoop': 633, 'forgot': 634, 'indentedi': 635, 'ware': 636, 'visibl': 637, 'trash': 638, 'switch': 639, 'variou': 640, 'unless': 641, 'nation': 642, 'geograph': 643, 'magazin': 644, 'oh': 645, 'nd': 646, 'pri': 647, 'sens': 648, 'request': 649, 'watch': 650, 'id': 651, 'rip': 652, 'caus': 653, 'unappealingli': 654, 'children': 655, 'five': 656, 'significantli': 657, 'straighten': 658, 'walla': 659, 'floati': 660, 'girlfriend': 661, 'liter': 662, 'hundr': 663, 'unreason': 664, 'perfectthi': 665, 'discov': 666, 'molli': 667, 'paininth': 668, 'keyboard': 669, 'modif': 670, 'regrett': 671, 'lucki': 672, 'sent': 673, 'joke': 674, 'leastr': 675, 'biggera': 676, 'goe': 677, 'throw': 678, 'itll': 679, 'nobodi': 680, 'obvious': 681, 'youll': 682, 'reconfigur': 683, 'entir': 684, 'nifti': 685, 'knock': 686, 'fragil': 687, 'import': 688, 'booksth': 689, 'understand': 690, 'earth': 691, 'mess': 692, 'defect': 693, 'problemalso': 694, 'didt': 695, 'detail': 696, 'tall': 697, 'tip': 698, 'gentli': 699, 'itreal': 700, 'wrong': 701, 'contain': 702, 'useless': 703, 'fold': 704, 'bare': 705, 'wo': 706, 'brush': 707, 'multipurpos': 708, 'sad': 709, 'mei': 710, 'beam': 711, 'run': 712, 'thru': 713, 'basement': 714, 'studio': 715, 'thrill': 716, 'magnet': 717, 'itthem': 718, 'beamdidnt': 719, 'degre': 720, 'immedi': 721, 'admit': 722, 'that': 723, 'adjust': 724, 'theth': 725, 'seat': 726, 'rigid': 727, 'formi': 728, 'fals': 729, 'post': 730, 'wast': 731, 'refund': 732, 'postag': 733, 'worthless': 734, 'hate': 735, 'screwsso': 736, 'singleton': 737, 'live': 738, 'apart': 739, 'sooo': 740, 'itwish': 741, 'sold': 742, 'trip': 743, 'depotmi': 744, 'dingi': 745, 'logic': 746, 'vertic': 747, 'hide': 748, 'front': 749, 'partbut': 750, 'irrit': 751, 'sleep': 752, 'thattwo': 753, 'execut': 754, 'wife': 755, 'slant': 756, 'taller': 757, 'stabil': 758, 'sidebysid': 759, 'agre': 760, 'ore': 761, 'coollook': 762, 'proport': 763, 'averag': 764, 'format': 765, 'coveri': 766, 'cute': 767, 'cup': 768, 'silicon': 769, 'cap': 770, 'almost': 771, 'imposs': 772, 'layer': 773, 'glass': 774, 'crack': 775, 'local': 776, 'shop': 777, 'onlin': 778, 'capac': 779, 'ridicul': 780, 'split': 781, 'droop': 782, 'hardbound': 783, 'somehow': 784, 'someon': 785, 'readi': 786, 'workthank': 787, 'medium': 788, 'materiali': 789, 'largei': 790, 'pull': 791, 'wallboard': 792, 'outor': 793, 'foot': 794, 'fun': 795, 'next': 796, 'deeper': 797, 'ledg': 798, 'strang': 799, 'negat': 800, 'shelfsinc': 801, 'decid': 802, 'avoid': 803, 'bother': 804, 'uninstal': 805, 'leav': 806, 'wallsmayb': 807, 'superb': 808, 'elegantli': 809, 'nook': 810, 'laptop': 811, 'night': 812, 'stand': 813, 'refer': 814, 'airport': 815, 'router': 816, 'storag': 817, 'bedroom': 818, 'shape': 819, 'terrif': 820, 'organ': 821, 'accomplish': 822, 'reccokmend': 823, 'friend': 824, 'talk': 825, 'th': 826, 'grade': 827, 'us': 828, 'dvd': 829, 'cabl': 830, 'draw': 831, 'attent': 832, 'deal': 833, 'fill': 834, 'babi': 835, 'tool': 836, 'extraordinari': 837, 'saver': 838, 'mysteri': 839, 'appear': 840, 'unexpect': 841, 'plant': 842, 'showi': 843, 'along': 844, 'collag': 845, 'add': 846, 'threedimension': 847, 'aspect': 848, 'instil': 849, 'promptli': 850, 'ton': 851, 'outcom': 852, 'guest': 853, 'funni': 854, 'convo': 855, 'worri': 856, 'eleg': 857, 'manner': 858, 'lover': 859, 'upgrad': 860, 'studfind': 861, 'ft': 862, 'ground': 863, 'gaudi': 864, 'theme': 865, 'nurseri': 866, 'robust': 867, 'appar': 868, 'air': 869, 'simpl': 870, 'condit': 871, 'honestli': 872, 'yetbut': 873, 'strongi': 874, 'gonna': 875, 'creat': 876, 'midst': 877, 'hour': 878, 'follow': 879, 'guidelin': 880, 'showcas': 881, 'literari': 882, 'amazinga': 883, 'saverit': 884, 'expact': 885, 'cours': 886, 'brought': 887, 'grandma': 888, 'freak': 889, 'wizard': 890, 'kill': 891, 'battl': 892, 'wallhold': 893, 'never': 894, 'ugli': 895, 'empti': 896, 'everyon': 897, 'interest': 898, 'android': 899, 'boyfriend': 900, 'law': 901, 'window': 902, 'list': 903, 'steadi': 904, 'digit': 905, 'smallm': 906, 'desk': 907, 'unusu': 908, 'accent': 909, 'prop': 910, 'shown': 911}\n",
      "\n",
      "Training sequences:\n",
      " [[282, 110, 63, 283, 161, 2, 2, 63, 110, 79, 432, 219, 284, 30, 39, 2, 27, 93, 94, 433, 111, 434, 8, 435, 162, 12, 80, 44, 34, 436, 163, 7, 8, 81, 11, 220, 94, 136, 164, 5, 56, 35, 437, 94, 41, 2], [56, 285, 54, 18, 64, 65, 64, 165, 438, 12, 82, 55, 2, 221, 439, 83, 65, 440], [441, 442, 18, 13, 45, 57, 46, 84, 2, 27, 66, 58, 166, 137, 20, 167, 443, 2, 14, 6, 10, 444, 64, 36, 45, 85, 168, 15, 59, 18, 445, 446, 169, 286, 138, 447, 222], [448, 25, 14, 46, 55, 2, 37, 60, 13, 449, 14, 450, 451, 6, 20, 5, 27, 223, 36, 287, 65, 112, 2, 58, 95, 5, 223, 41, 2, 10, 113, 224, 67, 81, 138, 3, 31, 225, 452, 453, 2], [288, 54, 96, 68, 226, 2, 69, 289, 5, 2, 114, 27, 454, 2, 27, 29, 70, 290, 455, 97, 20, 71, 60, 2, 86], [42, 72, 18, 20, 139, 456, 30, 46, 55, 2, 227, 457, 17, 2, 6, 28, 458, 139, 115, 6, 291, 140, 98, 170, 459, 13, 6, 43, 5, 13, 2, 26, 460], [21, 171, 7, 32, 8, 34, 141, 59, 292, 87, 5, 32, 461, 462], [37, 86, 172, 293, 16, 12, 463, 173, 19, 294, 2, 28, 34, 66, 14, 60, 13, 2, 33, 58, 464, 28, 6, 142, 47, 65, 2, 57, 39, 55, 465, 137, 47, 295, 96], [466, 228, 143, 18, 22, 64, 99, 116, 143, 9, 79, 229, 20, 3, 174, 117, 2, 467, 468, 469, 41, 2, 88, 296, 69], [297, 230, 6, 470, 175, 471, 2, 37, 18, 55, 118, 58, 142, 298, 299, 300, 301, 4, 113, 64, 141, 144, 15, 44, 34, 51, 73, 472], [176, 9, 117, 113, 473, 474, 114, 2, 475, 476, 27, 224, 48], [26, 48, 45, 32, 4, 477, 119, 302, 2, 21, 478, 303, 2, 479, 177, 304, 43, 17], [100, 41, 61, 37, 7, 34, 11, 33, 5, 2, 32, 304, 5, 27, 28, 2, 480, 2, 68, 30, 305, 8, 88, 178, 120, 29, 481, 43, 17, 39, 2, 27, 121, 179, 482, 87, 45, 483, 4, 306], [73, 484, 7, 22, 178, 88, 40, 290, 231, 5, 8, 485, 7, 49, 180, 82, 231, 227, 87, 111, 89], [19, 80, 232, 145, 13, 65, 486, 487, 179, 181], [24, 14, 307, 112, 2, 2, 52, 9, 14, 488, 6, 489, 122, 27, 308, 19, 2, 46, 225, 9, 20, 309, 490, 112, 491, 74, 33, 111, 310, 17, 182, 41, 2, 311, 51, 312], [313, 29, 5, 50, 183, 50, 184, 37, 223], [492, 8, 24, 314, 185, 146, 10, 21, 147, 233, 493, 101, 18, 302, 20, 5, 84, 2, 123, 46, 84, 2, 148, 114, 39, 14, 315, 18, 2, 186, 187, 13, 14, 146, 6, 40, 316, 69, 75, 48, 8, 2, 18, 188, 233, 79, 124, 35, 494], [85, 31, 495, 117, 124, 4, 234, 60, 317, 496, 497, 8, 57, 2, 2, 18, 39, 56, 13, 235, 318, 189, 2, 102, 236, 39, 50, 498, 499, 74, 98, 500, 76, 180, 103], [188, 21, 76, 7], [14, 2, 190, 149, 24, 501, 4, 73, 46, 186, 502, 191], [22, 5, 88, 29, 177, 2, 503, 230, 6, 192, 2, 319, 146, 96, 150, 237, 99, 504, 110, 63, 192, 21, 238, 239, 10, 19, 505, 506, 76, 7, 320, 507], [57, 46, 2, 300, 301], [6, 179, 289, 14, 240, 93, 27, 84, 2, 175, 2, 93, 2, 151, 58, 77, 508, 5, 321, 150, 6, 14, 2, 152, 46, 13, 84, 84, 13, 322, 241, 509], [22, 52, 25, 90, 178, 52, 25, 5, 2, 323, 35, 111, 510, 2, 239, 97, 104, 25, 324, 89, 17, 66, 5, 2, 323, 95, 511, 171, 68, 512, 513, 2, 4, 20, 193, 514, 103, 52, 9, 180], [72, 325, 326, 52, 153, 193, 104, 153, 99, 327, 76, 11, 104, 328, 105, 95, 39, 58, 82, 180, 8, 515, 11, 516, 329, 6, 82, 60, 11, 106, 517, 76, 518, 232, 41, 76, 329, 102, 21, 519, 520, 15, 52, 43, 14, 11, 9, 82], [85, 36, 153, 18, 9, 154, 66, 14, 57, 55, 2, 9, 20, 14, 55, 521, 194, 522, 330, 331, 2, 46, 55, 2, 20, 14, 155, 523, 62, 18, 9, 40, 524, 525, 526, 154], [125, 24, 527, 15, 242, 528, 20, 126, 529, 195, 127, 4, 21, 15, 326, 128, 530, 127, 243, 244, 66, 196, 531, 15, 162, 197, 332, 54, 532], [288, 15, 245, 78, 77, 333, 161, 74, 324, 21, 170, 334, 170, 129, 335, 21, 43, 5, 63, 2, 17, 2, 102, 533, 21, 53, 4, 151, 17, 165, 2], [85, 31, 77, 534, 61, 198, 336, 535, 219, 74, 176, 41, 13, 20, 10, 50, 41, 2, 22, 5, 194, 115, 536, 10, 57, 2, 21, 537, 337, 2], [146, 76, 2, 57, 2, 338, 7, 337, 2, 194], [538, 234, 138, 62], [50, 199, 182, 61, 150, 96, 539, 29, 2, 151, 171], [42, 64, 130, 339, 246, 96, 32], [540, 57, 2, 100, 163, 78, 4, 3, 10, 26, 32, 33, 91, 153], [340, 33, 541, 124, 293, 69, 24, 195, 43, 8, 542, 87, 151, 70, 2, 8, 543, 544, 11, 320, 105, 20, 545, 15, 2, 546, 7, 56, 54, 231, 221, 35, 33, 70], [119, 6, 9, 11, 247, 9, 131, 107, 11, 547, 101, 548, 11, 32, 54, 11, 111, 21, 200, 11, 107, 4, 341, 549, 226, 91, 98, 197], [342, 13, 2, 118, 35, 4, 141, 550, 6, 108, 343], [551, 169, 2, 114, 39, 14, 6, 75, 344, 95, 39, 47, 2, 97, 345, 122, 346, 552, 347, 553, 59, 17, 76, 7, 554, 75, 10, 248], [42, 64, 9, 201, 140, 12, 44, 56, 555, 249, 67, 556, 69], [31, 557, 115, 2, 7, 335, 71, 120, 558, 145, 559, 8, 6, 560, 250, 33, 67, 561, 348, 129, 562, 563, 349, 564], [565, 566, 95, 7, 35, 4, 567, 14, 568, 91, 2, 350, 569, 13, 53, 351, 116, 570, 7, 571, 107, 111, 89, 2], [62, 244, 9, 32, 50, 251, 18, 2, 144, 572, 350, 18, 2, 349, 38, 244, 9, 573, 15, 81, 13, 81, 13, 2], [16, 12, 141, 13, 2, 25, 175, 14, 2, 27, 15, 36, 287, 69], [48, 352, 353, 30, 187, 574, 29, 63, 575, 25, 25, 102, 89, 37, 11, 82], [26, 89, 118, 5, 354, 2, 156, 127, 132, 5, 143, 355, 143, 2, 20, 576, 70, 96, 7, 17, 354, 156], [3, 577, 120, 33, 5, 100, 29, 19, 356, 55, 2, 28, 578, 35, 29, 202, 103, 579, 2, 580, 581], [582, 9, 4, 203, 583, 584, 168, 20, 199, 120, 102, 157, 39, 20, 357, 19, 252, 35, 10, 585, 13, 203], [22, 18, 13, 186, 129, 37, 60, 2, 181, 132], [327, 586, 344, 145, 95, 84, 2, 204, 73, 114, 587, 13, 588, 589, 2, 14, 28, 2, 150, 590, 306, 8, 591, 592, 220, 27, 28, 2, 358, 44, 157, 87, 35, 593], [22, 103, 9, 106, 594, 198, 133, 37, 86, 595, 138, 245, 48, 596, 242, 46, 235, 195, 54, 253], [20, 17, 147, 2, 597, 254, 359, 205, 126, 20, 5, 254], [334, 101, 44, 80, 127, 106, 49, 11, 198], [22, 25, 9, 206, 123, 12, 75, 51, 39, 355, 44, 19, 11, 598, 69, 207, 62, 360], [361, 109], [599, 208, 147, 2, 38, 71, 209, 177, 41, 2, 208, 5, 63, 283, 2, 161], [33, 5, 50, 600, 219], [130, 133, 601, 602, 68, 119], [125, 6, 87, 157, 209, 603, 604, 29, 184, 125, 210, 50, 8], [85, 98, 10, 46, 13, 2, 19, 167, 41, 2, 14, 28], [89, 255, 209, 211, 256, 61, 10, 605, 2, 49], [115, 7, 2, 6, 43, 315, 257, 2, 33, 72, 362], [363, 21, 120, 33, 5, 364, 50, 184, 74, 33, 5, 18, 2, 17, 257, 187, 606, 53, 148, 29, 13, 4, 73, 92, 86, 98, 157, 184, 43, 5, 41, 2], [26, 92, 235, 109, 101, 15, 26, 361, 48, 26, 365, 7, 26, 19, 607, 127, 608], [11, 359, 197, 366, 17, 7, 136, 37, 8], [25, 9, 10, 9, 6, 206, 609, 130, 67, 70, 93, 75, 10, 67, 8, 11, 18, 258, 75, 10, 6, 89, 610, 200, 6, 7, 367, 9, 2, 11, 97, 259, 52, 153, 41, 260, 7, 19, 6, 32, 74, 260, 11, 56, 19, 251, 109, 19, 611, 612, 307, 613, 70, 9, 6, 15, 614, 65, 2, 204, 368, 2, 62, 261, 91, 104, 6, 5, 110, 52, 3, 31, 26, 21, 52, 75, 10], [125, 31, 615, 9, 262, 284, 81], [40, 369, 134, 47, 308, 93, 2, 27, 65, 616, 35, 25, 92, 202, 22, 65, 18, 104, 122, 93, 2, 154, 363, 617, 92, 212, 53, 618, 263, 370, 81, 619, 210, 264, 2], [265, 620, 249], [6, 10, 252], [43, 15, 10], [2, 263, 253, 115, 36, 25, 621, 104, 6, 53, 89, 5, 29, 622, 234, 83, 52, 25, 19, 50, 41, 87, 128, 7, 126, 2], [37, 80, 34, 49, 623, 6, 185, 236, 253, 25, 5, 110, 29, 371], [186, 624, 191, 26, 14, 187, 224, 10, 113], [104, 7, 2, 6, 172, 70, 625, 123, 626, 232, 7, 258, 11, 366, 372, 171, 7, 11, 6, 129, 73, 373], [148, 139, 5, 2, 119], [51, 627, 49, 17, 628], [42, 174, 629, 374, 21, 630, 631, 42, 140, 49, 44, 11, 375, 632], [633, 634, 191, 150, 50, 199, 19], [10, 3, 120, 9, 11, 258, 635, 376, 17, 377, 56, 54, 122, 636, 213], [70, 637, 50, 256], [53, 126, 378, 2, 92, 31, 265, 83, 379, 207, 209, 638], [6, 10, 113, 222, 154, 53, 57, 2, 4, 17, 19, 60, 13, 188, 639], [16, 12, 11, 2, 640, 13, 107, 5], [641, 8, 380, 165, 59, 70, 255, 126, 317, 378, 29], [169, 369, 47, 28, 2, 381, 382, 19, 294, 13, 71, 31, 8], [17, 642, 643, 644, 9, 262, 192, 383, 645, 74, 38, 228, 44, 80, 266, 169, 54, 342, 38, 384, 251, 54, 385, 52, 158], [309, 7, 248, 97, 37, 143, 29], [124, 135, 267, 268, 646, 24, 75, 176, 18, 18, 94, 386, 12, 375, 34, 7, 176, 19, 60, 94, 14, 112, 13, 2, 214, 135, 387, 269, 94, 47, 388, 44, 151, 389, 7, 647, 34, 388, 12, 135, 132, 255, 269, 38, 119, 128, 35, 648, 40, 649, 269, 94, 125, 24, 650, 13, 651, 314, 173, 24, 9, 173, 267, 268], [22, 99, 5, 88, 178, 9, 11, 33, 14, 34, 123, 652, 6, 215, 197, 7, 233, 37, 63, 2, 653, 192, 654], [5, 655, 2, 32], [21, 5, 2, 141], [42, 656, 9, 106, 44, 19, 17, 390, 56, 62, 19, 44, 9, 25, 657, 206, 40, 270, 658, 39, 35, 14, 659, 379, 62], [191, 2, 391, 660, 22, 661, 4, 662, 663, 2, 9, 18, 39, 27, 14, 6, 58, 53, 7, 380, 137, 239, 5, 28, 27, 68, 43, 2, 204, 102, 77, 664], [71], [154, 22, 286, 138, 665, 38, 119], [22, 103, 25, 17, 9, 241, 247, 666, 667, 11, 140, 392, 352, 16, 393, 668], [23, 271, 394, 395, 9, 396, 77, 72, 397, 78, 23, 182, 61, 196], [357, 61, 129, 17, 2, 30, 121, 28, 2, 142, 260, 19, 115, 6, 5, 669, 14, 398, 670, 14, 398], [42, 671, 266, 106, 11, 333, 399, 124, 101, 51, 11], [92, 31, 400, 59, 311, 401, 136, 346, 672, 39, 167], [402, 15, 128, 206, 87, 11, 402, 92, 31, 43, 249, 673, 11, 674], [45, 26, 362, 42, 403, 45, 299, 83, 9, 675, 18, 122, 27, 2, 297, 6, 37, 47, 370, 121, 47, 97, 98, 77, 404, 298, 77, 676, 95, 5, 2, 677, 59, 3, 678, 9, 2, 405, 2, 121, 679, 5, 21, 680, 406, 15, 681, 185, 16, 112, 45, 35, 270, 28, 2, 220, 134, 48, 117, 28, 2, 682, 683, 684, 59, 8, 73, 685, 407, 5, 57, 55, 2, 28, 99, 368, 165, 2, 121, 408, 121, 16, 686, 155, 59, 86, 33, 72, 390, 17, 246, 687, 688], [66, 5, 46, 13, 689, 155, 272, 28, 137, 295, 240, 93, 2, 77, 142, 146, 5, 27, 6, 332, 2, 4, 21, 36, 271, 28, 137, 175, 409, 28, 27, 66, 58, 142, 33, 690, 2, 322, 73, 46, 312, 410, 691, 692, 15, 693, 42, 136, 9, 694, 695, 131, 411, 44, 42, 140, 9, 11, 163, 356, 69], [24, 696, 228, 6, 5, 63, 30, 237, 229, 30, 63, 237, 697, 4, 21, 698, 37, 65, 313, 2, 107, 336, 699, 338, 700, 85, 238, 86, 4, 3, 88], [170, 701, 132, 243, 168, 71, 229, 385, 702, 213], [5, 257, 2], [85, 24, 241, 51, 44, 216, 7, 703], [48, 49, 7, 5, 704, 273, 705, 5, 273, 706, 274, 412, 707, 273, 274, 152, 6, 2, 708], [709, 710, 412, 193, 18, 711, 712, 713, 714, 715, 716, 31, 43, 405, 2, 6, 8, 157, 717, 216, 718, 719, 10, 215, 720, 721, 215, 67, 17, 2, 722, 340, 53, 8, 723, 48], [43, 724, 358, 35, 725, 726, 310, 727, 728, 19], [248, 24, 729, 118, 13, 60, 13, 730, 207, 135, 731, 134, 732, 200, 80, 733], [734], [735, 59, 6, 51, 736, 384, 737, 738, 739, 48, 16, 351, 740, 185, 741, 742, 11, 96, 80, 261, 743, 190, 744, 6, 130, 133, 745, 4, 148, 292, 21, 31, 746, 162, 747, 90, 6, 66, 196, 133, 748, 749, 133, 750, 152, 107, 751, 152, 133, 43, 752, 152, 753, 173, 238, 321, 99, 265, 754], [755, 22, 116, 22, 275, 158, 275, 158, 102, 113, 9, 145, 345, 756, 118, 364, 4, 275, 158, 179, 15, 216, 7, 50, 757, 72, 409, 236, 319, 158, 66, 758, 21, 8, 147, 2, 382, 262, 20, 413, 144, 15, 103, 52, 114, 64, 759], [376, 760, 124, 282, 194, 38, 761, 110, 210, 762, 38, 403, 373, 37, 391, 210, 122, 27, 2, 4, 71, 30, 2, 763, 14, 414, 764, 765, 404, 83, 32, 39, 47, 766, 22, 103, 303, 106, 8, 266], [767, 92, 768, 769, 770, 771, 772, 15, 8, 240, 773, 774, 775, 4, 54, 245, 9, 776, 777, 193, 778], [12, 296, 16, 202, 183, 347, 29, 779, 5, 73, 32, 780, 30, 88, 291, 15, 247, 30, 372, 781, 256], [12, 34, 7, 782, 29, 17, 41, 783, 2, 188, 90, 155, 34, 406, 415, 123, 348], [22, 189, 374, 784, 785, 367, 201, 33, 181, 33, 416, 40, 270, 35, 2, 166, 2, 274, 250, 276, 17, 2, 277, 40, 786, 8, 136, 15, 787], [5, 29, 101, 20, 252, 788, 13, 2], [26, 69, 118, 14, 112, 13, 84, 2, 62], [242, 276, 131, 25, 243, 789, 98, 401, 22, 162, 2, 790, 107, 67, 79, 791, 792, 793, 97, 794], [85, 172, 795, 79, 7, 16, 58, 2, 48, 117, 796], [208, 225, 42, 797, 9, 325, 417, 2, 14, 28, 798, 53, 8, 799, 198, 800, 155, 272, 15, 7, 801, 386, 12, 802, 47, 8, 417, 7, 6, 803, 804, 805, 806, 76, 807, 51, 81, 161, 60, 199, 2], [9, 278, 31, 202, 62, 38], [23, 19], [23], [808], [23, 90, 204, 55, 2, 809], [23, 16, 12], [23], [23, 117, 810], [10, 23, 139, 17, 30, 2, 149], [23, 7, 365], [47, 811, 812, 813, 17, 407, 23, 418, 30, 814, 2, 108, 19, 68, 78, 305, 8], [23, 815, 316, 816, 7, 49], [23, 419, 17], [2, 6, 217, 40, 420, 15, 214, 2, 36, 7, 40, 15, 23, 817], [23, 271, 394, 395, 9, 396, 77, 72, 397, 78, 23, 182, 61, 196], [278, 105, 261, 353, 2], [23, 213, 90, 264, 59, 40, 42], [23, 408, 49, 818], [38, 23, 108, 61, 38, 106, 134, 32, 127, 3, 819, 40, 164, 250, 38, 131, 48, 421, 135, 820, 24], [392, 278, 62, 416, 821, 2, 7, 4, 3, 35, 4, 21, 36, 68, 159, 24, 31], [415, 23, 16, 12, 822, 108, 48, 5, 147, 2, 4, 71, 20, 54, 823, 824], [279, 825, 272, 218, 91, 16, 12, 4, 71, 8, 826, 827, 189, 2, 10, 23, 422, 828, 30, 829, 830, 201, 423], [212, 78, 15, 100, 424, 24, 831, 832, 246, 3, 53, 833, 177, 6, 105, 834, 418, 7, 149, 3, 31, 3, 109], [835, 217, 145, 836, 393, 15, 116, 837, 3, 105, 838, 839, 2, 30, 840, 841, 68, 78], [3, 24, 16, 12, 425, 211, 2, 842, 4, 3], [26, 212, 83, 3, 109, 15, 100, 426], [23, 90, 843, 2, 844, 845, 149, 846, 125, 847, 848], [10, 3, 2, 4, 159, 7], [3, 24], [3, 24, 8, 213, 218, 410, 2, 4, 3, 47, 58], [3, 24, 16, 849], [10, 3, 16, 12, 6], [10, 280, 131, 850, 3, 109], [16, 12, 4, 3, 10, 61], [4, 3, 15, 851, 426], [4, 3, 16, 12, 8, 259, 34, 425, 32], [16, 49, 4, 3], [26, 21, 31, 852, 4, 3, 5, 88], [129, 3, 36, 2, 7, 19, 30, 2], [16, 12, 4, 279, 853, 419, 420, 214, 2, 36, 7, 427, 854, 855, 128], [3, 24, 131, 134], [3], [281, 856, 2, 28, 20, 389, 25, 148, 227, 25, 387, 90, 2, 857, 858, 16, 12, 74, 51, 411, 11, 3], [10, 3, 16, 12], [42, 81, 13, 36, 45, 428, 2, 859, 4, 3, 7, 222, 42, 100, 2], [4, 3, 74, 160, 51, 34, 183, 78, 860], [159, 10, 26, 32, 75, 67, 12, 34, 51, 259, 34, 12, 3, 4], [8, 861, 8, 377, 15, 45, 12, 116, 862, 863, 4, 341, 277, 5, 18, 2, 221, 4, 864, 2, 25, 172, 128, 3, 205, 190, 263], [3, 205, 330, 331, 865, 866, 160, 16, 12], [22, 174, 10, 3], [867, 83, 422, 5, 211, 2, 38, 68, 868, 36, 869, 870, 83, 16, 12, 281, 109, 195, 38, 130, 429, 343, 3, 871, 108, 156, 40, 26, 164, 360, 135], [872, 139, 8, 873, 130, 429, 4, 3, 874, 72, 875, 4, 159], [8, 876, 6, 877, 2, 126, 383, 2, 10, 3, 339, 82, 8, 208, 203, 79, 79, 167, 166, 2, 6], [22, 99, 90, 64, 18, 2, 9, 6, 215, 7, 414, 878, 116, 413, 144, 879, 880, 216, 82, 6, 26, 279, 3, 53, 881, 882, 264], [281, 45, 16, 12, 3, 2, 38, 47, 166, 36, 3, 205, 91], [25, 16, 49, 4, 3, 144], [10, 3], [883, 3, 105, 884, 885, 421, 207], [8, 45, 254, 8, 34, 886, 11, 160, 40, 164, 132, 427], [10, 3, 4, 3, 78, 399, 424, 134], [887, 888, 79, 36, 45, 889, 168, 890, 67, 891, 892, 3, 8, 105, 86], [10, 26, 32, 4, 3, 893, 174, 211, 2, 894, 230], [22, 226, 276, 280, 4, 895, 896, 26, 20, 8, 101, 214, 428, 72, 3], [10, 3, 4, 277, 7, 160, 49, 381], [12, 280, 3, 430, 431, 897, 51, 190, 898], [8, 49, 899, 423, 201, 7, 10, 3], [6, 10, 3, 189, 900, 8, 30, 901, 318, 2, 40, 56, 56, 285, 132, 183, 5, 100, 29, 123], [3, 24], [9, 400, 31, 22, 17, 218, 91, 163, 902, 4, 3, 10, 32], [16, 12, 160, 4, 217, 4, 217, 149, 903, 212, 38, 3, 430, 431], [26, 904, 371, 4, 3], [200, 218, 91, 5, 2, 328, 905, 10, 61, 4, 3], [3, 24, 159, 267, 268], [36, 2, 30, 4, 3, 5, 906, 2], [36, 45, 4, 3, 907, 4, 108, 156, 16, 12, 4, 3], [16, 12, 4, 3], [908, 3, 909, 910], [108, 156, 911, 203, 16, 12, 37, 86, 181, 80, 4, 3], [4, 3], [10, 3]]\n",
      "\n",
      "Padded training sequences:\n",
      " [[282 110  63 ...   0   0   0]\n",
      " [ 56 285  54 ...   0   0   0]\n",
      " [441 442  18 ...   0   0   0]\n",
      " ...\n",
      " [108 156 911 ...   0   0   0]\n",
      " [  4   3   0 ...   0   0   0]\n",
      " [ 10   3   0 ...   0   0   0]]\n",
      "\n",
      "Padded training shape: (205, 93)\n",
      "Training sequences data type: <class 'list'>\n",
      "Padded Training sequences data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize our training data\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "# Get our training data word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "\n",
    "# Pad the training sequences\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
    "\n",
    "# Output the results of our work\n",
    "print(\"Word index:\\n\", word_index)\n",
    "print(\"\\nTraining sequences:\\n\", train_sequences)\n",
    "print(\"\\nPadded training sequences:\\n\", train_padded)\n",
    "print(\"\\nPadded training shape:\", train_padded.shape)\n",
    "print(\"Training sequences data type:\", type(train_sequences))\n",
    "print(\"Padded Training sequences data type:\", type(train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call less lb max depth book book lb less see signific downward slope stack enough book cover back bracket left feel uncomfort use especi sinc instal includ hardwar anchor spun around wall use differ screw secur bracket cant happi hold go make l bracket larger book -> [282 110  63 283 161   2   2  63 110  79 432 219 284  30  39   2  27  93\n",
      "  94 433 111 434   8 435 162  12  80  44  34 436 163   7   8  81  11 220\n",
      "  94 136 164   5  56  35 437  94  41   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "go ahead buy larg set small set paperback imoi instal stud hardcov book start wobblesolid design small thin -> [ 56 285  54  18  64  65  64 165 438  12  82  55   2 221 439  83  65 440\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bookhold platform larg size bookshelf big standard hardback book cover doesnt reach invis lip would find overs book fit shelf work optioni set float bookshelf love thought get thing larg lesson learn found anoth brand stock job -> [441 442  18  13  45  57  46  84   2  27  66  58 166 137  20 167 443   2\n",
      "  14   6  10 444  64  36  45  85 168  15  59  18 445 446 169 286 138 447\n",
      " 222   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "forewarn shelv fit standard hardcov book even smaller size tricki fit prong undersid shelf would hold cover close float effect small normal book reach far hold close larger book work fine otherwis tri differ brand great idea dimens skew everyday book -> [448  25  14  46  55   2  37  60  13 449  14 450 451   6  20   5  27 223\n",
      "  36 287  65 112   2  58  95   5 223  41   2  10 113 224  67  81 138   3\n",
      "  31 225 452 453   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "excit buy hang place son book disappoint suppos hold book wide cover child book cover weight bend scare eventu fall would good smaller book though -> [288  54  96  68 226   2  69 289   5   2 114  27 454   2  27  29  70 290\n",
      " 455  97  20  71  60   2  86   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order think larg would abl accommod stack standard hardcov book case technic put book shelf bottom deep abl conceal shelf might miss wish descript differenti size shelf could hold size book realli bum -> [ 42  72  18  20 139 456  30  46  55   2 227 457  17   2   6  28 458 139\n",
      " 115   6 291 140  98 170 459  13   6  43   5  13   2  26 460   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "like stay wall well use anchor right thing impress metal hold well salti climat -> [ 21 171   7  32   8  34 141  59 292  87   5  32 461 462   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "even though unit quit easi instal give star need specif book bottom anchor doesnt fit smaller size book dont reach length bottom shelf hook keep small book big enough hardcov softcov lip keep slip hang -> [ 37  86 172 293  16  12 463 173  19 294   2  28  34  66  14  60  13   2\n",
      "  33  58 464  28   6 142  47  65   2  57  39  55 465 137  47 295  96   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "ad state pound larg bought set three hung pound one see true would great sever read book besid bed couch larger book cookbook super disappoint -> [466 228 143  18  22  64  99 116 143   9  79 229  20   3 174 117   2 467\n",
      " 468 469  41   2  88 296  69   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "main issu shelf self long regular book even larg hardcov actual reach hook underneath basic defeat purpos look fine set right recommend get hardwar anchor come pretti cheap -> [297 230   6 470 175 471   2  37  18  55 118  58 142 298 299 300 301   4\n",
      " 113  64 141 144  15  44  34  51  73 472   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realiz one read fine print gotta wide book hit bottomback cover otherwis want -> [176   9 117 113 473 474 114   2 475 476  27 224  48   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli want bookshelf well look elsewher return meant book like tear first book foundat limit term could put -> [ 26  48  45  32   4 477 119 302   2  21 478 303   2 479 177 304  43  17\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "lot larger expect even wall anchor screw dont hold book well term hold cover bottom book quantiti book place stack plan use cookbook kitchen howev weight restrict could put enough book cover top part silver metal bookshelf arm look tacki -> [100  41  61  37   7  34  11  33   5   2  32 304   5  27  28   2 480   2\n",
      "  68  30 305   8  88 178 120  29 481  43  17  39   2  27 121 179 482  87\n",
      "  45 483   4 306   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "pretti weak wall bought kitchen cookbook im scare may hold use dri wall mount instead stud may case metal feel flimsi -> [ 73 484   7  22 178  88  40 290 231   5   8 485   7  49 180  82 231 227\n",
      "  87 111  89   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "need includ drill bit size small sheet paper part instruct -> [ 19  80 232 145  13  65 486 487 179 181   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "product fit sort normal book book singl one fit onto shelf regardless hard cover soft need book standard dimens one would stick awkwardli normal bookcas also dont feel comfort put heavier larger book ever come across -> [ 24  14 307 112   2   2  52   9  14 488   6 489 122  27 308  19   2  46\n",
      " 225   9  20 309 490 112 491  74  33 111 310  17 182  41   2 311  51 312\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "amount weight hold much definit much claim even close -> [313  29   5  50 183  50 184  37 223   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "havent use product rate isnt base work like mani other assum said larg meant would hold hardback book easili standard hardback book wasnt wide enough fit take larg book guess textbook size fit base shelf im extrem disappoint didnt want use book larg hope other see review make mistak -> [492   8  24 314 185 146  10  21 147 233 493 101  18 302  20   5  84   2\n",
      " 123  46  84   2 148 114  39  14 315  18   2 186 187  13  14 146   6  40\n",
      " 316  69  75  48   8   2  18 188 233  79 124  35 494   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love idea ponder read review look compar smaller type avail unabl use big book book larg enough go size high school year book seem stabl enough much offset weighti also wish four hole instead two -> [ 85  31 495 117 124   4 234  60 317 496 497   8  57   2   2  18  39  56\n",
      "  13 235 318 189   2 102 236  39  50 498 499  74  98 500  76 180 103   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "hope like hole wall -> [188  21  76   7   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "fit book home pictur product page look pretti standard guess verifi measur -> [ 14   2 190 149  24 501   4  73  46 186 502 191   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought hold cookbook weight limit book lenght issu shelf sag book wider base hang end inch three wich less lb sag like concept wont work need sadli patch hole wall extra bummer -> [ 22   5  88  29 177   2 503 230   6 192   2 319 146  96 150 237  99 504\n",
      " 110  63 192  21 238 239  10  19 505 506  76   7 320 507   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "big standard book defeat purpos -> [ 57  46   2 300 301   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelf part suppos fit insid back cover hardback book long book back book wouldnt reach littl tab hold cut end shelf fit book know standard size hardback hardback size width open spine -> [  6 179 289  14 240  93  27  84   2 175   2  93   2 151  58  77 508   5\n",
      " 321 150   6  14   2 152  46  13  84  84  13 322 241 509   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought singl shelv display kitchen singl shelv hold book straight make feel confid book wont fall doubl shelv felt flimsi put doesnt hold book straight far everyth stay place noth uneven book look would rather opt two singl one instead -> [ 22  52  25  90 178  52  25   5   2 323  35 111 510   2 239  97 104  25\n",
      " 324  89  17  66   5   2 323  95 511 171  68 512 513   2   4  20 193 514\n",
      " 103  52   9 180   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "think gotten multipl singl bookshelv rather doubl bookshelv three horizont hole screw doubl arent space far enough reach stud instead use longer screw reinforc center shelf stud smaller screw came outer hole involv drill larger hole center seem like itd easier get singl could fit screw one stud -> [ 72 325 326  52 153 193 104 153  99 327  76  11 104 328 105  95  39  58\n",
      "  82 180   8 515  11 516 329   6  82  60  11 106 517  76 518 232  41  76\n",
      " 329 102  21 519 520  15  52  43  14  11   9  82   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love float bookshelv larg one problem doesnt fit big hardcov book one would fit hardcov graphic novel illustr harri potter book standard hardcov book would fit whole reason purchas larg one im curiou anyon els problem -> [ 85  36 153  18   9 154  66  14  57  55   2   9  20  14  55 521 194 522\n",
      " 330 331   2  46  55   2  20  14 155 523  62  18   9  40 524 525 526 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "nice product costli get manufactur cost would support retail price packag look like get multipl piec per packag pack individu doesnt matter color get sinc complet hidden buy cheapest -> [125  24 527  15 242 528  20 126 529 195 127   4  21  15 326 128 530 127\n",
      " 243 244  66 196 531  15 162 197 332  54 532   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "excit get new hous littl upset depth also felt like descript mislead descript made sound like could hold lb book put book seem unstabl like way look wouldnt put paperback book -> [288  15 245  78  77 333 161  74 324  21 170 334 170 129 335  21  43   5\n",
      "  63   2  17   2 102 533  21  53   4 151  17 165   2   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love idea littl flimsier expect kind bounc lean downward also realiz larger size would work much larger book bought hold novel conceal featur work big book like art text book -> [ 85  31  77 534  61 198 336 535 219  74 176  41  13  20  10  50  41   2\n",
      "  22   5 194 115 536  10  57   2  21 537 337   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "base hole book big book touch wall text book novel -> [146  76   2  57   2 338   7 337   2 194   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "junk compar brand purchas -> [538 234 138  62   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "much bigger heavier expect end hang afraid weight book wouldnt stay -> [ 50 199 182  61 150  96 539  29   2 151 171   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order set arriv without anyth hang well -> [ 42  64 130 339 246  96  32   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "tad big book lot around hous look great work realli well dont room bookshelv -> [540  57   2 100 163  78   4   3  10  26  32  33  91 153   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "usual dont write review quit disappoint product price could use thicker metal wouldnt bend book use counter sunk screw extra space would help get book flush wall go buy may start make dont bend -> [340  33 541 124 293  69  24 195  43   8 542  87 151  70   2   8 543 544\n",
      "  11 320 105  20 545  15   2 546   7  56  54 231 221  35  33  70   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "return shelf one screw second one receiv still screw husband said aww screw well buy screw feel like got screw still look absolut gorgeou son room wish complet -> [119   6   9  11 247   9 131 107  11 547 101 548  11  32  54  11 111  21\n",
      " 200  11 107   4 341 549 226  91  98 197   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "particular size book actual make look right decent shelf exactli promis -> [342  13   2 118  35   4 141 550   6 108 343   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "suck found book wide enough fit shelf didnt extend far enough keep book fall forward hard believ consum test done thing put hole wall figur didnt work advertis -> [551 169   2 114  39  14   6  75 344  95  39  47   2  97 345 122 346 552\n",
      " 347 553  59  17  76   7 554  75  10 248   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order set one box miss instal hardwar go spend money tri match disappoint -> [ 42  64   9 201 140  12  44  56 555 249  67 556  69   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "idea behind conceal book wall sound good howev applic bit flaw use shelf workbut pleas dont tri reus move made sturdiest materi consid warn -> [ 31 557 115   2   7 335  71 120 558 145 559   8   6 560 250  33  67 561\n",
      " 348 129 562 563 349 564   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "imo sit far wall make look awkward fit flow room book least minimum size way fix hung hollow wall hanger still feel flimsi book -> [565 566  95   7  35   4 567  14 568  91   2 350 569  13  53 351 116 570\n",
      "   7 571 107 111  89   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "purchas individu one well much better larg book recommend own least larg book consid item individu one ideal get differ size differ size book -> [ 62 244   9  32  50 251  18   2 144 572 350  18   2 349  38 244   9 573\n",
      "  15  81  13  81  13   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal right size book shelv long fit book cover get float effect disappoint -> [ 16  12 141  13   2  25 175  14   2  27  15  36 287  69   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "want quick solut stack textbook altogeth weight lb spread shelv shelv seem flimsi even screw stud -> [ 48 352 353  30 187 574  29  63 575  25  25 102  89  37  11  82   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli flimsi actual hold weightsiz book describ packag say hold pound correct pound book would accur bend hang wall put weightsiz describ -> [ 26  89 118   5 354   2 156 127 132   5 143 355 143   2  20 576  70  96\n",
      "   7  17 354 156   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great theori howev dont hold lot weight need huge hardcov book bottom thu make weight ive two dump book floor unceremoni -> [  3 577 120  33   5 100  29  19 356  55   2  28 578  35  29 202 103 579\n",
      "   2 580 581   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "ye one look photo known reader thought would bigger howev seem strong enough would meet need ok make work half size photo -> [582   9   4 203 583 584 168  20 199 120 102 157  39  20 357  19 252  35\n",
      "  10 585  13 203   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought larg size guess made even smaller book instruct say -> [ 22  18  13 186 129  37  60   2 181 132   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "horizont plane extend bit far hardback book collect pretti wide rang size coffe tabl book fit bottom book end clumsi tacki use rubber band secur cover bottom book suffici hardwar strong metal make trustworthi -> [327 586 344 145  95  84   2 204  73 114 587  13 588 589   2  14  28   2\n",
      " 150 590 306   8 591 592 220  27  28   2 358  44 157  87  35 593   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought two one came ship kind scratch even though supposedli brand new want inferior manufactur standard high price buy umbra -> [ 22 103   9 106 594 198 133  37  86 595 138 245  48 596 242  46 235 195\n",
      "  54 253   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "would put mani book surround speaker provid addit support would hold speaker -> [ 20  17 147   2 597 254 359 205 126  20   5 254   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "mislead said hardwar includ packag came mount screw kind -> [334 101  44  80 127 106  49  11 198   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought shelv one bent easili instal didnt come enough correct hardwar need screw onlycamewith disappoint amazon purchas experi -> [ 22  25   9 206 123  12  75  51  39 355  44  19  11 598  69 207  62 360\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "expens qualiti -> [361 109   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "sometim check mani book item good carri limit larger book check hold lb max book depth -> [599 208 147   2  38  71 209 177  41   2 208   5  63 283   2 161   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "dont hold much tilt downward -> [ 33   5  50 600 219   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "arriv scratch finish worn place return -> [130 133 601 602  68 119   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "nice shelf metal strong carri anywher near weight claim nice show much use -> [125   6  87 157 209 603 604  29 184 125 210  50   8   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love wish work standard size book need find larger book fit bottom -> [ 85  98  10  46  13   2  19 167  41   2  14  28   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "flimsi cannot carri heavi load expect work coupl book mount -> [ 89 255 209 211 256  61  10 605   2  49   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "conceal wall book shelf could take lightweight book dont think practic -> [115   7   2   6  43 315 257   2  33  72 362   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "overal like howev dont hold nearli much claim also dont hold larg book put lightweight textbook flex way wasnt weight size look pretti cool though wish strong claim could hold larger book -> [363  21 120  33   5 364  50 184  74  33   5  18   2  17 257 187 606  53\n",
      " 148  29  13   4  73  92  86  98 157 184  43   5  41   2   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli cool high qualiti said get realli expens want realli decor wall realli need offer packag discount -> [ 26  92 235 109 101  15  26 361  48  26 365   7  26  19 607 127 608   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "screw provid complet broke put wall cant even use -> [ 11 359 197 366  17   7 136  37   8   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelv one work one shelf bent upon arriv tri bend back didnt work tri use screw larg head didnt work shelf flimsi final got shelf wall took one book screw fall drywal singl bookshelv larger plate wall need shelf well also plate screw go need better qualiti need backer rod sort prevent bend one shelf get okay small book collect light book purchas save room doubl shelf hold less singl great idea realli like singl didnt work -> [ 25   9  10   9   6 206 609 130  67  70  93  75  10  67   8  11  18 258\n",
      "  75  10   6  89 610 200   6   7 367   9   2  11  97 259  52 153  41 260\n",
      "   7  19   6  32  74 260  11  56  19 251 109  19 611 612 307 613  70   9\n",
      "   6  15 614  65   2 204 368   2  62 261  91 104   6   5 110  52   3  31\n",
      "  26  21  52  75  10   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "nice idea studi one side slope differ -> [125  31 615   9 262 284  81   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "im difficult time keep soft back book cover small bump make shelv cool ive bought small larg doubl hard back book problem overal tho cool uniqu way kick offic someth differ displayand show favorit book -> [ 40 369 134  47 308  93   2  27  65 616  35  25  92 202  22  65  18 104\n",
      " 122  93   2 154 363 617  92 212  53 618 263 370  81 619 210 264   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "poor valu money -> [265 620 249   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelf work ok -> [  6  10 252   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "could get work -> [43 15 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "book offic umbra conceal float shelv unfortun doubl shelf way flimsi hold weight obviou compar design singl shelv need much larger metal piec wall support book -> [  2 263 253 115  36  25 621 104   6  53  89   5  29 622 234  83  52  25\n",
      "  19  50  41  87 128   7 126   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "even includ anchor mount kit shelf isnt stabl umbra shelv hold less weight reliabl -> [ 37  80  34  49 623   6 185 236 253  25   5 110  29 371   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "guess shouldv measur realli fit textbook otherwis work fine -> [186 624 191  26  14 187 224  10 113   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "doubl wall book shelf unit bend aw easili care drill wall head screw broke remaind stay wall screw shelf made pretti poorli -> [104   7   2   6 172  70 625 123 626 232   7 258  11 366 372 171   7  11\n",
      "   6 129  73 373   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "wasnt abl hold book return -> [148 139   5   2 119   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "come direct mount put togeth -> [ 51 627  49  17 628   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order sever month ago like themhowev last order miss mount hardwar screw plastic jacket -> [ 42 174 629 374  21 630 631  42 140  49  44  11 375 632   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "whoop forgot measur end much bigger need -> [633 634 191 150  50 199  19   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great howev one screw head indentedi couldnt put screwdriv go buy hard ware store -> [ 10   3 120   9  11 258 635 376  17 377  56  54 122 636 213   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bend visibl much load -> [ 70 637  50 256   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "way support substanti book cool idea poor design frustrat amazon carri trash -> [ 53 126 378   2  92  31 265  83 379 207 209 638   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelf work fine job problem way big book look put need smaller size hope switch -> [  6  10 113 222 154  53  57   2   4  17  19  60  13 188 639   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal screw book variou size still hold -> [ 16  12  11   2 640  13 107   5   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "unless use tini paperback thing bend cannot support type substanti weight -> [641   8 380 165  59  70 255 126 317 378  29   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "found difficult keep bottom book correctli mayb need specif size good idea use -> [169 369  47  28   2 381 382  19 294  13  71  31   8   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "put nation geograph magazin one side sag lower oh also item state hardwar includ none found buy particular item your better buy cheaper singl version -> [ 17 642 643 644   9 262 192 383 645  74  38 228  44  80 266 169  54 342\n",
      "  38 384 251  54 385  52 158   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "stick wall advertis fall even pound weight -> [309   7 248  97  37 143  29   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "review seller custom servic nd product didnt realiz larg larg bracket alreadi instal plastic anchor wall realiz need smaller bracket fit normal size book ask seller let exchang bracket keep current hardwar wouldnt damag wall pri anchor current instal seller say cannot exchang item return piec make sens im request exchang bracket nice product watch size id rate star product one star custom servic -> [124 135 267 268 646  24  75 176  18  18  94 386  12 375  34   7 176  19\n",
      "  60  94  14 112  13   2 214 135 387 269  94  47 388  44 151 389   7 647\n",
      "  34 388  12 135 132 255 269  38 119 128  35 648  40 649 269  94 125  24\n",
      " 650  13 651 314 173  24   9 173 267 268   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought three hold cookbook kitchen one screw dont fit anchor easili rip shelf fell complet wall other even lb book caus sag unappealingli -> [ 22  99   5  88 178   9  11  33  14  34 123 652   6 215 197   7 233  37\n",
      "  63   2 653 192 654   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "hold children book well -> [  5 655   2  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "like hold book right -> [ 21   5   2 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order five one came hardwar need put ill go purchas need hardwar one shelv significantli bent im sure straighten enough make fit walla frustrat purchas -> [ 42 656   9 106  44  19  17 390  56  62  19  44   9  25 657 206  40 270\n",
      " 658  39  35  14 659 379  62   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "measur book intend floati bought girlfriend look liter hundr book one larg enough cover fit shelf reach way wall tini lip wont hold bottom cover place could book collect seem littl unreason -> [191   2 391 660  22 661   4 662 663   2   9  18  39  27  14   6  58  53\n",
      "   7 380 137 239   5  28  27  68  43   2 204 102  77 664   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "good -> [71  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "problem bought anoth brand perfectthi item return -> [154  22 286 138 665  38 119   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought two shelv put one open second discov molli screw miss turn quick easi project paininth -> [ 22 103  25  17   9 241 247 666 667  11 140 392 352  16 393 668   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect except pain chip one section littl think bang hous perfect heavier expect matter -> [ 23 271 394 395   9 396  77  72 397  78  23 182  61 196   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "meet expect made put book stack top bottom book hook plate need conceal shelf hold keyboard fit bill modif fit bill -> [357  61 129  17   2  30 121  28   2 142 260  19 115   6   5 669  14 398\n",
      " 670  14 398   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "order regrett none came screw upset peopl review said come screw -> [ 42 671 266 106  11 333 399 124 101  51  11   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "cool idea coolest thing ever seen cant believ lucki enough find -> [ 92  31 400  59 311 401 136 346 672  39 167   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "kid get piec bent metal screw kid cool idea could money sent screw joke -> [402  15 128 206  87  11 402  92  31  43 249 673  11 674   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bookshelf realli practic order function bookshelf basic design one leastr larg hard cover book main shelf even keep someth top keep fall wish littl clip underneath littl biggera far hold book goe thing great throw one book pile book top itll hold like nobodi busi get obvious isnt easi normal bookshelf make sure bottom book secur time want read bottom book youll reconfigur entir thing use pretti nifti nightstand hold big hardcov book bottom three light paperback book top lamp top easi knock whole thing though dont think ill put anyth fragil import -> [ 45  26 362  42 403  45 299  83   9 675  18 122  27   2 297   6  37  47\n",
      " 370 121  47  97  98  77 404 298  77 676  95   5   2 677  59   3 678   9\n",
      "   2 405   2 121 679   5  21 680 406  15 681 185  16 112  45  35 270  28\n",
      "   2 220 134  48 117  28   2 682 683 684  59   8  73 685 407   5  57  55\n",
      "   2  28  99 368 165   2 121 408 121  16 686 155  59  86  33  72 390  17\n",
      " 246 687 688]\n",
      "doesnt hold standard size booksth whole point bottom lip slip insid back book littl hook base hold cover shelf hidden book look like float except bottom lip long therefor bottom cover doesnt reach hook dont understand book width pretti standard across board earth mess get defect order cant one problemalso didt receiv necessari hardwar order miss one screw around huge disappoint -> [ 66   5  46  13 689 155 272  28 137 295 240  93   2  77 142 146   5  27\n",
      "   6 332   2   4  21  36 271  28 137 175 409  28  27  66  58 142  33 690\n",
      "   2 322  73  46 312 410 691 692  15 693  42 136   9 694 695 131 411  44\n",
      "  42 140   9  11 163 356  69   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "product detail state shelf hold lb stack inch true stack lb inch tall look like tip even small amount book still bounc gentli touch itreal love concept though look great cookbook -> [ 24 696 228   6   5  63  30 237 229  30  63 237 697   4  21 698  37  65\n",
      " 313   2 107 336 699 338 700  85 238  86   4   3  88   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "descript wrong say pack thought good true cheaper contain store -> [170 701 132 243 168  71 229 385 702 213   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "hold lightweight book -> [  5 257   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love product open come hardwar attach wall useless -> [ 85  24 241  51  44 216   7 703   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "want mount wall hold fold towel bare hold towel wo slide happen brush towel slide know shelf book multipurpos -> [ 48  49   7   5 704 273 705   5 273 706 274 412 707 273 274 152   6   2\n",
      " 708   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "sad mei happen rather larg beam run thru basement studio thrill idea could pile book shelf use strong magnet attach itthem beamdidnt work fell degre immedi fell tri put book admit usual way use that want -> [709 710 412 193  18 711 712 713 714 715 716  31  43 405   2   6   8 157\n",
      " 717 216 718 719  10 215 720 721 215  67  17   2 722 340  53   8 723  48\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "could adjust suffici make theth seat comfort rigid formi need -> [ 43 724 358  35 725 726 310 727 728  19   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "advertis product fals actual size smaller size post amazon seller wast time refund got includ postag -> [248  24 729 118  13  60  13 730 207 135 731 134 732 200  80 733   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "worthless -> [734   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "hate thing shelf come screwsso your singleton live apart want easi fix sooo isnt itwish sold screw hang includ save trip home depotmi shelf arriv scratch dingi look wasnt impress like idea logic sinc vertic display shelf doesnt matter scratch hide front scratch partbut know still irrit know scratch could sleep know thattwo star concept cut three poor execut -> [735  59   6  51 736 384 737 738 739  48  16 351 740 185 741 742  11  96\n",
      "  80 261 743 190 744   6 130 133 745   4 148 292  21  31 746 162 747  90\n",
      "   6  66 196 133 748 749 133 750 152 107 751 152 133  43 752 152 753 173\n",
      " 238 321  99 265 754   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "wife bought hung bought singlewid version singlewid version seem fine one bit forward slant actual nearli look singlewid version part get attach wall much taller think therefor stabl wider version doesnt stabil like use mani book mayb side would highli recommend get two singl wide set sidebysid -> [755  22 116  22 275 158 275 158 102 113   9 145 345 756 118 364   4 275\n",
      " 158 179  15 216   7  50 757  72 409 236 319 158  66 758  21   8 147   2\n",
      " 382 262  20 413 144  15 103  52 114  64 759   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "couldnt agre review call novel item ore less show coollook item function poorli even intend show hard cover book look good stack book proport fit within averag format clip design well enough keep coveri bought two first came use none -> [376 760 124 282 194  38 761 110 210 762  38 403 373  37 391 210 122  27\n",
      "   2   4  71  30   2 763  14 414 764 765 404  83  32  39  47 766  22 103\n",
      " 303 106   8 266   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "cute cool cup silicon cap almost imposs get use insid layer glass crack look buy new one local shop rather onlin -> [767  92 768 769 770 771 772  15   8 240 773 774 775   4  54 245   9 776\n",
      " 777 193 778   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "instal super easi ive definit test weight capac hold pretti well ridicul stack cookbook might get second stack remaind split load -> [ 12 296  16 202 183 347  29 779   5  73  32 780  30  88 291  15 247  30\n",
      " 372 781 256   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "instal anchor wall droop weight put larger hardbound book hope display whole anchor busi theyr easili move -> [ 12  34   7 782  29  17  41 783   2 188  90 155  34 406 415 123 348   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought year ago somehow someon took box dont instruct dont yet im sure make book invis book slide pleas tell put book beauti im readi use cant get workthank -> [ 22 189 374 784 785 367 201  33 181  33 416  40 270  35   2 166   2 274\n",
      " 250 276  17   2 277  40 786   8 136  15 787   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "hold weight said would ok medium size book -> [  5  29 101  20 252 788  13   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli disappoint actual fit normal size hardback book purchas -> [ 26  69 118  14 112  13  84   2  62   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "manufactur tell receiv shelv pack materiali wish seen bought sinc book largei still tri see pull wallboard outor fall foot -> [242 276 131  25 243 789  98 401  22 162   2 790 107  67  79 791 792 793\n",
      "  97 794   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "love unit fun see wall easi reach book want read next -> [ 85 172 795  79   7  16  58   2  48 117 796   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "check dimens order deeper one gotten typic book fit bottom ledg way use strang kind negat whole point get wall shelfsinc alreadi instal decid keep use typic wall shelf avoid bother uninstal leav hole wallsmayb come differ depth smaller bigger book -> [208 225  42 797   9 325 417   2  14  28 798  53   8 799 198 800 155 272\n",
      "  15   7 801 386  12 802  47   8 417   7   6 803 804 805 806  76 807  51\n",
      "  81 161  60 199   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "one best idea ive purchas item -> [  9 278  31 202  62  38   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect need -> [23 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "perfect -> [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "superb -> [808   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect display collect hardcov book elegantli -> [ 23  90 204  55   2 809   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect easi instal -> [23 16 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "perfect -> [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "perfect read nook -> [ 23 117 810   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work perfect abl put stack book pictur -> [ 10  23 139  17  30   2 149   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect wall decor -> [ 23   7 365   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "keep laptop night stand put nightstand perfect spot stack refer book exactli need place hous plan use -> [ 47 811 812 813  17 407  23 418  30 814   2 108  19  68  78 305   8   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect airport extrem router wall mount -> [ 23 815 316 816   7  49   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect area put -> [ 23 419  17   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "book shelf amaz im alway get ask book float wall im get perfect storag -> [  2   6 217  40 420  15 214   2  36   7  40  15  23 817   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect except pain chip one section littl think bang hous perfect heavier expect matter -> [ 23 271 394 395   9 396  77  72 397  78  23 182  61 196   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "best space save solut book -> [278 105 261 353   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect store display favorit thing im order -> [ 23 213  90 264  59  40  42   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect lamp mount bedroom -> [ 23 408  49 818   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "item perfect exactli expect item came time well packag great shape im happi pleas item receiv want thank seller terrif product -> [ 38  23 108  61  38 106 134  32 127   3 819  40 164 250  38 131  48 421\n",
      " 135 820  24   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "turn best purchas yet organ book wall look great make look like float place awesom product idea -> [392 278  62 416 821   2   7   4   3  35   4  21  36  68 159  24  31   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "theyr perfect easi instal accomplish exactli want hold mani book look good would buy reccokmend friend -> [415  23  16  12 822 108  48   5 147   2   4  71  20  54 823 824   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "neat talk point daughter room easi instal look good use th grade year book work perfect allow us stack dvd cabl box tv -> [279 825 272 218  91  16  12   4  71   8 826 827 189   2  10  23 422 828\n",
      "  30 829 830 201 423   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "uniqu hous get lot comment product draw attent anyth great way deal limit shelf space fill spot wall pictur great idea great qualiti -> [212  78  15 100 424  24 831 832 246   3  53 833 177   6 105 834 418   7\n",
      " 149   3  31   3 109   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "babi amaz bit tool project get hung extraordinari great space saver mysteri book stack appear unexpect place hous -> [835 217 145 836 393  15 116 837   3 105 838 839   2  30 840 841  68  78\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product easi instal held heavi book plant look great -> [  3  24  16  12 425 211   2 842   4   3   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli uniqu design great qualiti get lot compliment -> [ 26 212  83   3 109  15 100 426   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "perfect display showi book along collag pictur add nice threedimension aspect -> [ 23  90 843   2 844 845 149 846 125 847 848   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great book look awesom wall -> [ 10   3   2   4 159   7   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product -> [ 3 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "great product use store daughter board book look great keep reach -> [  3  24   8 213 218 410   2   4   3  47  58   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product easi instil -> [  3  24  16 849   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great easi instal shelf -> [10  3 16 12  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "work wonder receiv promptli great qualiti -> [ 10 280 131 850   3 109   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal look great work expect -> [16 12  4  3 10 61  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "look great get ton compliment -> [  4   3  15 851 426   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "look great easi instal use drywal anchor held well -> [  4   3  16  12   8 259  34 425  32   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi mount look great -> [16 49  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "realli like idea outcom look great hold cookbook -> [ 26  21  31 852   4   3   5  88   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "made great float book wall need stack book -> [129   3  36   2   7  19  30   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal look neat guest area alway ask book float wall lol funni convo piec -> [ 16  12   4 279 853 419 420 214   2  36   7 427 854 855 128   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product receiv time -> [  3  24 131 134   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great -> [3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "excel worri book bottom would damag shelv wasnt case shelv let display book eleg manner easi instal also come necessari screw great -> [281 856   2  28  20 389  25 148 227  25 387  90   2 857 858  16  12  74\n",
      "  51 411  11   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great easi instal -> [10  3 16 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "order differ size float bookshelf gift book lover look great wall job order lot book -> [ 42  81  13  36  45 428   2 859   4   3   7 222  42 100   2   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "look great also sturdi come anchor definit hous upgrad -> [  4   3  74 160  51  34 183  78 860   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "awesom work realli well didnt tri instal anchor come drywal anchor instal great look -> [159  10  26  32  75  67  12  34  51 259  34  12   3   4   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "use studfind use screwdriv get bookshelf instal hung ft ground look absolut beauti hold larg book start look gaudi book shelv unit piec great addit home offic -> [  8 861   8 377  15  45  12 116 862 863   4 341 277   5  18   2 221   4\n",
      " 864   2  25 172 128   3 205 190 263   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great addit harri potter theme nurseri sturdi easi instal -> [  3 205 330 331 865 866 160  16  12   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought sever work great -> [ 22 174  10   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "robust design allow hold heavi book item place appar float air simpl design easi instal excel qualiti price item arriv earli promis great condit exactli describ im realli happi experi seller -> [867  83 422   5 211   2  38  68 868  36 869 870  83  16  12 281 109 195\n",
      "  38 130 429 343   3 871 108 156  40  26 164 360 135   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "honestli abl use yetbut arriv earli look great strongi think gonna look awesom -> [872 139   8 873 130 429   4   3 874  72 875   4 159   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "use creat shelf midst book support lower book work great without stud use check photo see see find invis book shelf -> [  8 876   6 877   2 126 383   2  10   3 339  82   8 208 203  79  79 167\n",
      " 166   2   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought three display set larg book one shelf fell wall within hour hung highli recommend follow guidelin attach stud shelf realli neat great way showcas literari favorit -> [ 22  99  90  64  18   2   9   6 215   7 414 878 116 413 144 879 880 216\n",
      "  82   6  26 279   3  53 881 882 264   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "excel bookshelf easi instal great book item keep invis float great addit room -> [281  45  16  12   3   2  38  47 166  36   3 205  91   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelv easi mount look great recommend -> [ 25  16  49   4   3 144   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great -> [10  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "amazinga great space saverit expact thank amazon -> [883   3 105 884 885 421 207   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "use bookshelf speaker use anchor cours screw sturdi im happi say lol -> [  8  45 254   8  34 886  11 160  40 164 132 427   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great look great hous peopl comment time -> [ 10   3   4   3  78 399 424 134   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "brought grandma see float bookshelf freak thought wizard tri kill battl great use space though -> [887 888  79  36  45 889 168 890  67 891 892   3   8 105  86   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work realli well look great wallhold sever heavi book never issu -> [ 10  26  32   4   3 893 174 211   2 894 230   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "bought son tell wonder look ugli empti realli would use said ask gift think great -> [ 22 226 276 280   4 895 896  26  20   8 101 214 428  72   3   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "work great look beauti wall sturdi mount correctli -> [ 10   3   4 277   7 160  49 381   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "instal wonder great convers starter everyon come home interest -> [ 12 280   3 430 431 897  51 190 898   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "use mount android tv box wall work great -> [  8  49 899 423 201   7  10   3   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "shelf work great year boyfriend use stack law school book im go go ahead say definit hold lot weight easili -> [  6  10   3 189 900   8  30 901 318   2  40  56  56 285 132 183   5 100\n",
      "  29 123   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product -> [ 3 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "one coolest idea bought put daughter room around window look great work well -> [  9 400  31  22  17 218  91 163 902   4   3  10  32   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal sturdi look amaz look amaz pictur list uniqu item great convers starter -> [ 16  12 160   4 217   4 217 149 903 212  38   3 430 431   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "realli steadi reliabl look great -> [ 26 904 371   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "got daughter room hold book arent digit work expect look great -> [200 218  91   5   2 328 905  10  61   4   3   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "great product awesom custom servic -> [  3  24 159 267 268   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "float book stack look great hold smallm book -> [ 36   2  30   4   3   5 906   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "float bookshelf look great desk look exactli describ easi instal look great -> [ 36  45   4   3 907   4 108 156  16  12   4   3   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "easi instal look great -> [16 12  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "unusu great accent prop -> [908   3 909 910   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "exactli describ shown photo easi instal even though instruct includ look great -> [108 156 911 203  16  12  37  86 181  80   4   3   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "look great -> [4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "work great -> [10  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "Word index (for reference): {'<UNK>': 1, 'book': 2, 'great': 3, 'look': 4, 'hold': 5, 'shelf': 6, 'wall': 7, 'use': 8, 'one': 9, 'work': 10, 'screw': 11, 'instal': 12, 'size': 13, 'fit': 14, 'get': 15, 'easi': 16, 'put': 17, 'larg': 18, 'need': 19, 'would': 20, 'like': 21, 'bought': 22, 'perfect': 23, 'product': 24, 'shelv': 25, 'realli': 26, 'cover': 27, 'bottom': 28, 'weight': 29, 'stack': 30, 'idea': 31, 'well': 32, 'dont': 33, 'anchor': 34, 'make': 35, 'float': 36, 'even': 37, 'item': 38, 'enough': 39, 'im': 40, 'larger': 41, 'order': 42, 'could': 43, 'hardwar': 44, 'bookshelf': 45, 'standard': 46, 'keep': 47, 'want': 48, 'mount': 49, 'much': 50, 'come': 51, 'singl': 52, 'way': 53, 'buy': 54, 'hardcov': 55, 'go': 56, 'big': 57, 'reach': 58, 'thing': 59, 'smaller': 60, 'expect': 61, 'purchas': 62, 'lb': 63, 'set': 64, 'small': 65, 'doesnt': 66, 'tri': 67, 'place': 68, 'disappoint': 69, 'bend': 70, 'good': 71, 'think': 72, 'pretti': 73, 'also': 74, 'didnt': 75, 'hole': 76, 'littl': 77, 'hous': 78, 'see': 79, 'includ': 80, 'differ': 81, 'stud': 82, 'design': 83, 'hardback': 84, 'love': 85, 'though': 86, 'metal': 87, 'cookbook': 88, 'flimsi': 89, 'display': 90, 'room': 91, 'cool': 92, 'back': 93, 'bracket': 94, 'far': 95, 'hang': 96, 'fall': 97, 'wish': 98, 'three': 99, 'lot': 100, 'said': 101, 'seem': 102, 'two': 103, 'doubl': 104, 'space': 105, 'came': 106, 'still': 107, 'exactli': 108, 'qualiti': 109, 'less': 110, 'feel': 111, 'normal': 112, 'fine': 113, 'wide': 114, 'conceal': 115, 'hung': 116, 'read': 117, 'actual': 118, 'return': 119, 'howev': 120, 'top': 121, 'hard': 122, 'easili': 123, 'review': 124, 'nice': 125, 'support': 126, 'packag': 127, 'piec': 128, 'made': 129, 'arriv': 130, 'receiv': 131, 'say': 132, 'scratch': 133, 'time': 134, 'seller': 135, 'cant': 136, 'lip': 137, 'brand': 138, 'abl': 139, 'miss': 140, 'right': 141, 'hook': 142, 'pound': 143, 'recommend': 144, 'bit': 145, 'base': 146, 'mani': 147, 'wasnt': 148, 'pictur': 149, 'end': 150, 'wouldnt': 151, 'know': 152, 'bookshelv': 153, 'problem': 154, 'whole': 155, 'describ': 156, 'strong': 157, 'version': 158, 'awesom': 159, 'sturdi': 160, 'depth': 161, 'sinc': 162, 'around': 163, 'happi': 164, 'paperback': 165, 'invis': 166, 'find': 167, 'thought': 168, 'found': 169, 'descript': 170, 'stay': 171, 'unit': 172, 'star': 173, 'sever': 174, 'long': 175, 'realiz': 176, 'limit': 177, 'kitchen': 178, 'part': 179, 'instead': 180, 'instruct': 181, 'heavier': 182, 'definit': 183, 'claim': 184, 'isnt': 185, 'guess': 186, 'textbook': 187, 'hope': 188, 'year': 189, 'home': 190, 'measur': 191, 'sag': 192, 'rather': 193, 'novel': 194, 'price': 195, 'matter': 196, 'complet': 197, 'kind': 198, 'bigger': 199, 'got': 200, 'box': 201, 'ive': 202, 'photo': 203, 'collect': 204, 'addit': 205, 'bent': 206, 'amazon': 207, 'check': 208, 'carri': 209, 'show': 210, 'heavi': 211, 'uniqu': 212, 'store': 213, 'ask': 214, 'fell': 215, 'attach': 216, 'amaz': 217, 'daughter': 218, 'downward': 219, 'secur': 220, 'start': 221, 'job': 222, 'close': 223, 'otherwis': 224, 'dimens': 225, 'son': 226, 'case': 227, 'state': 228, 'true': 229, 'issu': 230, 'may': 231, 'drill': 232, 'other': 233, 'compar': 234, 'high': 235, 'stabl': 236, 'inch': 237, 'concept': 238, 'wont': 239, 'insid': 240, 'open': 241, 'manufactur': 242, 'pack': 243, 'individu': 244, 'new': 245, 'anyth': 246, 'second': 247, 'advertis': 248, 'money': 249, 'pleas': 250, 'better': 251, 'ok': 252, 'umbra': 253, 'speaker': 254, 'cannot': 255, 'load': 256, 'lightweight': 257, 'head': 258, 'drywal': 259, 'plate': 260, 'save': 261, 'side': 262, 'offic': 263, 'favorit': 264, 'poor': 265, 'none': 266, 'custom': 267, 'servic': 268, 'exchang': 269, 'sure': 270, 'except': 271, 'point': 272, 'towel': 273, 'slide': 274, 'singlewid': 275, 'tell': 276, 'beauti': 277, 'best': 278, 'neat': 279, 'wonder': 280, 'excel': 281, 'call': 282, 'max': 283, 'slope': 284, 'ahead': 285, 'anoth': 286, 'effect': 287, 'excit': 288, 'suppos': 289, 'scare': 290, 'might': 291, 'impress': 292, 'quit': 293, 'specif': 294, 'slip': 295, 'super': 296, 'main': 297, 'underneath': 298, 'basic': 299, 'defeat': 300, 'purpos': 301, 'meant': 302, 'first': 303, 'term': 304, 'plan': 305, 'tacki': 306, 'sort': 307, 'soft': 308, 'stick': 309, 'comfort': 310, 'ever': 311, 'across': 312, 'amount': 313, 'rate': 314, 'take': 315, 'extrem': 316, 'type': 317, 'school': 318, 'wider': 319, 'extra': 320, 'cut': 321, 'width': 322, 'straight': 323, 'felt': 324, 'gotten': 325, 'multipl': 326, 'horizont': 327, 'arent': 328, 'center': 329, 'harri': 330, 'potter': 331, 'hidden': 332, 'upset': 333, 'mislead': 334, 'sound': 335, 'bounc': 336, 'text': 337, 'touch': 338, 'without': 339, 'usual': 340, 'absolut': 341, 'particular': 342, 'promis': 343, 'extend': 344, 'forward': 345, 'believ': 346, 'test': 347, 'move': 348, 'consid': 349, 'least': 350, 'fix': 351, 'quick': 352, 'solut': 353, 'weightsiz': 354, 'correct': 355, 'huge': 356, 'meet': 357, 'suffici': 358, 'provid': 359, 'experi': 360, 'expens': 361, 'practic': 362, 'overal': 363, 'nearli': 364, 'decor': 365, 'broke': 366, 'took': 367, 'light': 368, 'difficult': 369, 'someth': 370, 'reliabl': 371, 'remaind': 372, 'poorli': 373, 'ago': 374, 'plastic': 375, 'couldnt': 376, 'screwdriv': 377, 'substanti': 378, 'frustrat': 379, 'tini': 380, 'correctli': 381, 'mayb': 382, 'lower': 383, 'your': 384, 'cheaper': 385, 'alreadi': 386, 'let': 387, 'current': 388, 'damag': 389, 'ill': 390, 'intend': 391, 'turn': 392, 'project': 393, 'pain': 394, 'chip': 395, 'section': 396, 'bang': 397, 'bill': 398, 'peopl': 399, 'coolest': 400, 'seen': 401, 'kid': 402, 'function': 403, 'clip': 404, 'pile': 405, 'busi': 406, 'nightstand': 407, 'lamp': 408, 'therefor': 409, 'board': 410, 'necessari': 411, 'happen': 412, 'highli': 413, 'within': 414, 'theyr': 415, 'yet': 416, 'typic': 417, 'spot': 418, 'area': 419, 'alway': 420, 'thank': 421, 'allow': 422, 'tv': 423, 'comment': 424, 'held': 425, 'compliment': 426, 'lol': 427, 'gift': 428, 'earli': 429, 'convers': 430, 'starter': 431, 'signific': 432, 'left': 433, 'uncomfort': 434, 'especi': 435, 'spun': 436, 'l': 437, 'imoi': 438, 'wobblesolid': 439, 'thin': 440, 'bookhold': 441, 'platform': 442, 'overs': 443, 'optioni': 444, 'lesson': 445, 'learn': 446, 'stock': 447, 'forewarn': 448, 'tricki': 449, 'prong': 450, 'undersid': 451, 'skew': 452, 'everyday': 453, 'child': 454, 'eventu': 455, 'accommod': 456, 'technic': 457, 'deep': 458, 'differenti': 459, 'bum': 460, 'salti': 461, 'climat': 462, 'give': 463, 'length': 464, 'softcov': 465, 'ad': 466, 'besid': 467, 'bed': 468, 'couch': 469, 'self': 470, 'regular': 471, 'cheap': 472, 'print': 473, 'gotta': 474, 'hit': 475, 'bottomback': 476, 'elsewher': 477, 'tear': 478, 'foundat': 479, 'quantiti': 480, 'restrict': 481, 'silver': 482, 'arm': 483, 'weak': 484, 'dri': 485, 'sheet': 486, 'paper': 487, 'onto': 488, 'regardless': 489, 'awkwardli': 490, 'bookcas': 491, 'havent': 492, 'assum': 493, 'mistak': 494, 'ponder': 495, 'avail': 496, 'unabl': 497, 'offset': 498, 'weighti': 499, 'four': 500, 'page': 501, 'verifi': 502, 'lenght': 503, 'wich': 504, 'sadli': 505, 'patch': 506, 'bummer': 507, 'tab': 508, 'spine': 509, 'confid': 510, 'everyth': 511, 'noth': 512, 'uneven': 513, 'opt': 514, 'longer': 515, 'reinforc': 516, 'outer': 517, 'involv': 518, 'itd': 519, 'easier': 520, 'graphic': 521, 'illustr': 522, 'reason': 523, 'curiou': 524, 'anyon': 525, 'els': 526, 'costli': 527, 'cost': 528, 'retail': 529, 'per': 530, 'color': 531, 'cheapest': 532, 'unstabl': 533, 'flimsier': 534, 'lean': 535, 'featur': 536, 'art': 537, 'junk': 538, 'afraid': 539, 'tad': 540, 'write': 541, 'thicker': 542, 'counter': 543, 'sunk': 544, 'help': 545, 'flush': 546, 'husband': 547, 'aww': 548, 'gorgeou': 549, 'decent': 550, 'suck': 551, 'consum': 552, 'done': 553, 'figur': 554, 'spend': 555, 'match': 556, 'behind': 557, 'applic': 558, 'flaw': 559, 'workbut': 560, 'reus': 561, 'sturdiest': 562, 'materi': 563, 'warn': 564, 'imo': 565, 'sit': 566, 'awkward': 567, 'flow': 568, 'minimum': 569, 'hollow': 570, 'hanger': 571, 'own': 572, 'ideal': 573, 'altogeth': 574, 'spread': 575, 'accur': 576, 'theori': 577, 'thu': 578, 'dump': 579, 'floor': 580, 'unceremoni': 581, 'ye': 582, 'known': 583, 'reader': 584, 'half': 585, 'plane': 586, 'rang': 587, 'coffe': 588, 'tabl': 589, 'clumsi': 590, 'rubber': 591, 'band': 592, 'trustworthi': 593, 'ship': 594, 'supposedli': 595, 'inferior': 596, 'surround': 597, 'onlycamewith': 598, 'sometim': 599, 'tilt': 600, 'finish': 601, 'worn': 602, 'anywher': 603, 'near': 604, 'coupl': 605, 'flex': 606, 'offer': 607, 'discount': 608, 'upon': 609, 'final': 610, 'backer': 611, 'rod': 612, 'prevent': 613, 'okay': 614, 'studi': 615, 'bump': 616, 'tho': 617, 'kick': 618, 'displayand': 619, 'valu': 620, 'unfortun': 621, 'obviou': 622, 'kit': 623, 'shouldv': 624, 'aw': 625, 'care': 626, 'direct': 627, 'togeth': 628, 'month': 629, 'themhowev': 630, 'last': 631, 'jacket': 632, 'whoop': 633, 'forgot': 634, 'indentedi': 635, 'ware': 636, 'visibl': 637, 'trash': 638, 'switch': 639, 'variou': 640, 'unless': 641, 'nation': 642, 'geograph': 643, 'magazin': 644, 'oh': 645, 'nd': 646, 'pri': 647, 'sens': 648, 'request': 649, 'watch': 650, 'id': 651, 'rip': 652, 'caus': 653, 'unappealingli': 654, 'children': 655, 'five': 656, 'significantli': 657, 'straighten': 658, 'walla': 659, 'floati': 660, 'girlfriend': 661, 'liter': 662, 'hundr': 663, 'unreason': 664, 'perfectthi': 665, 'discov': 666, 'molli': 667, 'paininth': 668, 'keyboard': 669, 'modif': 670, 'regrett': 671, 'lucki': 672, 'sent': 673, 'joke': 674, 'leastr': 675, 'biggera': 676, 'goe': 677, 'throw': 678, 'itll': 679, 'nobodi': 680, 'obvious': 681, 'youll': 682, 'reconfigur': 683, 'entir': 684, 'nifti': 685, 'knock': 686, 'fragil': 687, 'import': 688, 'booksth': 689, 'understand': 690, 'earth': 691, 'mess': 692, 'defect': 693, 'problemalso': 694, 'didt': 695, 'detail': 696, 'tall': 697, 'tip': 698, 'gentli': 699, 'itreal': 700, 'wrong': 701, 'contain': 702, 'useless': 703, 'fold': 704, 'bare': 705, 'wo': 706, 'brush': 707, 'multipurpos': 708, 'sad': 709, 'mei': 710, 'beam': 711, 'run': 712, 'thru': 713, 'basement': 714, 'studio': 715, 'thrill': 716, 'magnet': 717, 'itthem': 718, 'beamdidnt': 719, 'degre': 720, 'immedi': 721, 'admit': 722, 'that': 723, 'adjust': 724, 'theth': 725, 'seat': 726, 'rigid': 727, 'formi': 728, 'fals': 729, 'post': 730, 'wast': 731, 'refund': 732, 'postag': 733, 'worthless': 734, 'hate': 735, 'screwsso': 736, 'singleton': 737, 'live': 738, 'apart': 739, 'sooo': 740, 'itwish': 741, 'sold': 742, 'trip': 743, 'depotmi': 744, 'dingi': 745, 'logic': 746, 'vertic': 747, 'hide': 748, 'front': 749, 'partbut': 750, 'irrit': 751, 'sleep': 752, 'thattwo': 753, 'execut': 754, 'wife': 755, 'slant': 756, 'taller': 757, 'stabil': 758, 'sidebysid': 759, 'agre': 760, 'ore': 761, 'coollook': 762, 'proport': 763, 'averag': 764, 'format': 765, 'coveri': 766, 'cute': 767, 'cup': 768, 'silicon': 769, 'cap': 770, 'almost': 771, 'imposs': 772, 'layer': 773, 'glass': 774, 'crack': 775, 'local': 776, 'shop': 777, 'onlin': 778, 'capac': 779, 'ridicul': 780, 'split': 781, 'droop': 782, 'hardbound': 783, 'somehow': 784, 'someon': 785, 'readi': 786, 'workthank': 787, 'medium': 788, 'materiali': 789, 'largei': 790, 'pull': 791, 'wallboard': 792, 'outor': 793, 'foot': 794, 'fun': 795, 'next': 796, 'deeper': 797, 'ledg': 798, 'strang': 799, 'negat': 800, 'shelfsinc': 801, 'decid': 802, 'avoid': 803, 'bother': 804, 'uninstal': 805, 'leav': 806, 'wallsmayb': 807, 'superb': 808, 'elegantli': 809, 'nook': 810, 'laptop': 811, 'night': 812, 'stand': 813, 'refer': 814, 'airport': 815, 'router': 816, 'storag': 817, 'bedroom': 818, 'shape': 819, 'terrif': 820, 'organ': 821, 'accomplish': 822, 'reccokmend': 823, 'friend': 824, 'talk': 825, 'th': 826, 'grade': 827, 'us': 828, 'dvd': 829, 'cabl': 830, 'draw': 831, 'attent': 832, 'deal': 833, 'fill': 834, 'babi': 835, 'tool': 836, 'extraordinari': 837, 'saver': 838, 'mysteri': 839, 'appear': 840, 'unexpect': 841, 'plant': 842, 'showi': 843, 'along': 844, 'collag': 845, 'add': 846, 'threedimension': 847, 'aspect': 848, 'instil': 849, 'promptli': 850, 'ton': 851, 'outcom': 852, 'guest': 853, 'funni': 854, 'convo': 855, 'worri': 856, 'eleg': 857, 'manner': 858, 'lover': 859, 'upgrad': 860, 'studfind': 861, 'ft': 862, 'ground': 863, 'gaudi': 864, 'theme': 865, 'nurseri': 866, 'robust': 867, 'appar': 868, 'air': 869, 'simpl': 870, 'condit': 871, 'honestli': 872, 'yetbut': 873, 'strongi': 874, 'gonna': 875, 'creat': 876, 'midst': 877, 'hour': 878, 'follow': 879, 'guidelin': 880, 'showcas': 881, 'literari': 882, 'amazinga': 883, 'saverit': 884, 'expact': 885, 'cours': 886, 'brought': 887, 'grandma': 888, 'freak': 889, 'wizard': 890, 'kill': 891, 'battl': 892, 'wallhold': 893, 'never': 894, 'ugli': 895, 'empti': 896, 'everyon': 897, 'interest': 898, 'android': 899, 'boyfriend': 900, 'law': 901, 'window': 902, 'list': 903, 'steadi': 904, 'digit': 905, 'smallm': 906, 'desk': 907, 'unusu': 908, 'accent': 909, 'prop': 910, 'shown': 911}\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(train_data, train_padded):\n",
    "  print('{} -> {}'.format(x, y))\n",
    "\n",
    "print(\"\\nWord index (for reference):\", word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#two hidden layers and 1 output layer\n",
    "model.add(Dense(35,activation='relu'))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.1436 - accuracy: 0.3659 - val_loss: 1.0871 - val_accuracy: 0.4878\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.0978 - accuracy: 0.4024 - val_loss: 1.0683 - val_accuracy: 0.4878\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.0583 - accuracy: 0.4329 - val_loss: 1.0541 - val_accuracy: 0.5610\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0286 - accuracy: 0.5061 - val_loss: 1.0419 - val_accuracy: 0.6341\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.0006 - accuracy: 0.5732 - val_loss: 1.0301 - val_accuracy: 0.6341\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9740 - accuracy: 0.6524 - val_loss: 1.0184 - val_accuracy: 0.6829\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.9496 - accuracy: 0.7073 - val_loss: 1.0057 - val_accuracy: 0.6585\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.9254 - accuracy: 0.7683 - val_loss: 0.9922 - val_accuracy: 0.6585\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.9012 - accuracy: 0.7988 - val_loss: 0.9786 - val_accuracy: 0.6829\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8765 - accuracy: 0.8293 - val_loss: 0.9639 - val_accuracy: 0.6829\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8526 - accuracy: 0.8476 - val_loss: 0.9480 - val_accuracy: 0.6829\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8279 - accuracy: 0.8659 - val_loss: 0.9314 - val_accuracy: 0.6829\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8031 - accuracy: 0.8841 - val_loss: 0.9148 - val_accuracy: 0.6829\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7777 - accuracy: 0.8963 - val_loss: 0.8978 - val_accuracy: 0.6829\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7523 - accuracy: 0.9085 - val_loss: 0.8800 - val_accuracy: 0.6829\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7270 - accuracy: 0.9085 - val_loss: 0.8612 - val_accuracy: 0.6829\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7021 - accuracy: 0.9207 - val_loss: 0.8424 - val_accuracy: 0.7073\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6775 - accuracy: 0.9268 - val_loss: 0.8239 - val_accuracy: 0.7073\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6531 - accuracy: 0.9268 - val_loss: 0.8062 - val_accuracy: 0.7073\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6285 - accuracy: 0.9268 - val_loss: 0.7893 - val_accuracy: 0.7561\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6048 - accuracy: 0.9268 - val_loss: 0.7720 - val_accuracy: 0.7561\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5815 - accuracy: 0.9268 - val_loss: 0.7558 - val_accuracy: 0.7805\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5579 - accuracy: 0.9268 - val_loss: 0.7401 - val_accuracy: 0.8049\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5353 - accuracy: 0.9268 - val_loss: 0.7252 - val_accuracy: 0.8293\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5130 - accuracy: 0.9390 - val_loss: 0.7110 - val_accuracy: 0.8293\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4913 - accuracy: 0.9451 - val_loss: 0.6973 - val_accuracy: 0.8293\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4699 - accuracy: 0.9451 - val_loss: 0.6843 - val_accuracy: 0.8049\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4491 - accuracy: 0.9451 - val_loss: 0.6727 - val_accuracy: 0.8049\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4285 - accuracy: 0.9451 - val_loss: 0.6611 - val_accuracy: 0.8049\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4087 - accuracy: 0.9451 - val_loss: 0.6515 - val_accuracy: 0.7805\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3897 - accuracy: 0.9451 - val_loss: 0.6437 - val_accuracy: 0.7561\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3706 - accuracy: 0.9451 - val_loss: 0.6350 - val_accuracy: 0.8049\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3524 - accuracy: 0.9451 - val_loss: 0.6260 - val_accuracy: 0.7805\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3347 - accuracy: 0.9451 - val_loss: 0.6171 - val_accuracy: 0.7805\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3174 - accuracy: 0.9451 - val_loss: 0.6082 - val_accuracy: 0.7805\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3007 - accuracy: 0.9634 - val_loss: 0.6007 - val_accuracy: 0.7561\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2843 - accuracy: 0.9695 - val_loss: 0.5936 - val_accuracy: 0.7317\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2691 - accuracy: 0.9695 - val_loss: 0.5861 - val_accuracy: 0.7317\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2543 - accuracy: 0.9756 - val_loss: 0.5796 - val_accuracy: 0.7561\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2402 - accuracy: 0.9817 - val_loss: 0.5722 - val_accuracy: 0.7561\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2270 - accuracy: 0.9878 - val_loss: 0.5653 - val_accuracy: 0.7561\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2144 - accuracy: 0.9878 - val_loss: 0.5580 - val_accuracy: 0.7561\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2027 - accuracy: 0.9878 - val_loss: 0.5534 - val_accuracy: 0.7317\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1917 - accuracy: 0.9878 - val_loss: 0.5490 - val_accuracy: 0.7317\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1813 - accuracy: 0.9878 - val_loss: 0.5442 - val_accuracy: 0.7317\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1716 - accuracy: 0.9878 - val_loss: 0.5414 - val_accuracy: 0.7317\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1625 - accuracy: 0.9878 - val_loss: 0.5397 - val_accuracy: 0.7317\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1540 - accuracy: 0.9878 - val_loss: 0.5377 - val_accuracy: 0.7317\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1463 - accuracy: 0.9878 - val_loss: 0.5362 - val_accuracy: 0.7317\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1385 - accuracy: 0.9939 - val_loss: 0.5355 - val_accuracy: 0.7805\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1319 - accuracy: 0.9939 - val_loss: 0.5353 - val_accuracy: 0.7805\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1253 - accuracy: 0.9939 - val_loss: 0.5353 - val_accuracy: 0.7805\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1195 - accuracy: 0.9939 - val_loss: 0.5361 - val_accuracy: 0.7805\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1136 - accuracy: 0.9939 - val_loss: 0.5375 - val_accuracy: 0.7805\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1083 - accuracy: 0.9939 - val_loss: 0.5388 - val_accuracy: 0.7805\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1034 - accuracy: 0.9939 - val_loss: 0.5400 - val_accuracy: 0.7805\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0987 - accuracy: 0.9939 - val_loss: 0.5423 - val_accuracy: 0.7805\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0945 - accuracy: 0.9939 - val_loss: 0.5432 - val_accuracy: 0.7805\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0902 - accuracy: 0.9939 - val_loss: 0.5448 - val_accuracy: 0.7805\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0863 - accuracy: 0.9939 - val_loss: 0.5469 - val_accuracy: 0.7805\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0828 - accuracy: 0.9939 - val_loss: 0.5493 - val_accuracy: 0.7805\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0794 - accuracy: 0.9939 - val_loss: 0.5527 - val_accuracy: 0.7805\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0762 - accuracy: 0.9939 - val_loss: 0.5557 - val_accuracy: 0.7805\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0731 - accuracy: 0.9939 - val_loss: 0.5573 - val_accuracy: 0.7805\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0702 - accuracy: 0.9939 - val_loss: 0.5585 - val_accuracy: 0.7805\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0674 - accuracy: 0.9939 - val_loss: 0.5604 - val_accuracy: 0.7805\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0648 - accuracy: 0.9939 - val_loss: 0.5630 - val_accuracy: 0.7805\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0623 - accuracy: 0.9939 - val_loss: 0.5650 - val_accuracy: 0.7805\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0599 - accuracy: 0.9939 - val_loss: 0.5677 - val_accuracy: 0.7805\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0578 - accuracy: 0.9939 - val_loss: 0.5702 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17921cd5160>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train,\n",
    "          validation_data=(x_test,y_test),\n",
    "          batch_size=100,epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 0, 0, 2, 0, 0, 2, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 2,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  1]\n",
      " [ 0  4  0]\n",
      " [ 1  6 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.40      1.00      0.57         4\n",
      "           2       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.80        41\n",
      "   macro avg       0.75      0.85      0.75        41\n",
      "weighted avg       0.88      0.80      0.82        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performane(y_test,predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 500)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fshape = 1\n",
    "x = x.reshape((x.shape[0], x.shape[1], n_fshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 3s 391ms/step - loss: 1.0985 - accuracy: 0.3854\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 3s 412ms/step - loss: 1.0974 - accuracy: 0.3854\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 3s 408ms/step - loss: 1.0966 - accuracy: 0.3854\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 3s 386ms/step - loss: 1.0954 - accuracy: 0.3854\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 3s 406ms/step - loss: 1.0937 - accuracy: 0.3854\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 3s 404ms/step - loss: 1.0909 - accuracy: 0.4146\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 3s 373ms/step - loss: 1.0848 - accuracy: 0.4390\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 3s 409ms/step - loss: nan - accuracy: 0.4293\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 3s 436ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 3s 403ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 3s 411ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 3s 390ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 3s 392ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 3s 380ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 3s 391ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 3s 366ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 3s 361ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 3s 391ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 3s 396ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 2s 356ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 3s 367ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 3s 360ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 3s 375ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 3s 383ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 3s 374ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 3s 359ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 3s 366ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 3s 390ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 3s 365ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 3s 361ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 2s 353ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 3s 402ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 2s 352ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 3s 389ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 2s 354ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 3s 361ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 3s 380ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 3s 373ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 4s 620ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 3s 399ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 3s 485ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 4s 535ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 3s 497ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 3s 380ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 3s 449ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 3s 477ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 2s 356ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 3s 364ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 3s 427ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 3s 360ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 4s 551ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 4s 606ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 3s 458ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 3s 499ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 3s 358ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 3s 360ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 3s 359ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 3s 363ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 3s 402ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 4s 531ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 3s 402ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 3s 360ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 3s 372ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 3s 466ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 4s 532ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 3s 372ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 3s 372ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 3s 365ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 3s 364ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 3s 394ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 3s 372ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 3s 367ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 3s 369ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 3s 374ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 3s 365ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 3s 369ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 3s 367ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 3s 369ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 3s 370ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 3s 377ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 3s 367ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 3s 393ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 3s 368ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 3s 373ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 3s 374ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 3s 372ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 3s 373ms/step - loss: nan - accuracy: 0.3854\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 3s 374ms/step - loss: nan - accuracy: 0.3854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1792b95b730>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_features ,n_fshape)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2], y=[2 2 0 0 1 2 1 0 0 2 1 0 1 2 1 1 0 0 2 1 0 2 1 2 2 1 2 0 1 1 0 1 1 0 2 2 2\n",
      " 0 2 0 1 1 1 0 0 1 0 0 0 1 2 1 2 1 2 2 2 1 0 0 0 1 2 2 1 0 0 2 2 2 2 2 2 0\n",
      " 1 1 1 2 1 0 2 0 2 0 0 1 1 1 1 0 0 2 2 2 2 0 0 0 1 0 2 1 1 2 2 2 1 1 0 1 1\n",
      " 0 0 1 2 0 2 0 0 1 1 1 0 2 0 0 0 1 0 0 0 0 1 2 2 0 1 2 0 0 2 2 1 0 2 0 0 0\n",
      " 2 1 0 2 2 1 1 1 1 2 0 1 0 1 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "#class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights1 = {0:0.92655367, 1:1.01234568, 2:1.07189542}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0]\n",
      " [ 4  0  0]\n",
      " [ 6  7  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        20\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       1.00      0.24      0.38        17\n",
      "\n",
      "    accuracy                           0.59        41\n",
      "   macro avg       0.56      0.41      0.39        41\n",
      "weighted avg       0.74      0.59      0.55        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc1 = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', class_weight = class_weights1)\n",
    "svc1.fit(x_train, y_train)\n",
    "\n",
    "svc_pred1 = svc1.predict(x_test)\n",
    "\n",
    "model_performane(y_test, svc_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
